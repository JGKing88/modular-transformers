{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjack-g-king\u001b[0m (\u001b[33mmodular_transformers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jackking/.netrc\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, BatchEncoding, GPT2LMHeadModel, GPT2Config, GPT2ForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch as torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "path = \"/om2/user/jackking/modular_transformers/scripts/dimensionality_reduction\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wandb.login(key=\"a338f755915cccd861b14f29bf68601d8e1ec2c9\")\n",
    "\n",
    "#set seed\n",
    "seed = 38\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, inputs, attn_mask=None, labels=None):\n",
    "        #cast to tensors if not already tensors\n",
    "        if not torch.is_tensor(inputs):\n",
    "            inputs = torch.tensor(inputs)\n",
    "        if not torch.is_tensor(labels):\n",
    "            labels = torch.tensor(labels)\n",
    "        if attn_mask is not None and not torch.is_tensor(attn_mask):\n",
    "            attn_mask = torch.tensor(attn_mask)\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        self.attn_mask = attn_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'attention_mask': self.attn_mask[idx]}\n",
    "        elif self.attn_mask is None:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        else:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'attention_mask': self.attn_mask[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        return item\n",
    "\n",
    "def make_autoregressive_dataset(data):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    dataset = tokenizer.batch_encode_plus(data, add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs = dataset[\"input_ids\"]\n",
    "    attn_mask = dataset[\"attention_mask\"]\n",
    "    labels = dataset[\"input_ids\"].clone()\n",
    "    context_len = inputs.size(1)\n",
    "    return LMDataset(inputs, attn_mask, labels), context_len\n",
    "\n",
    "def make_classification_dataset(data1, data2):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    len1 = len(data1)\n",
    "    len2 = len(data2)\n",
    "    combined = data1 + data2\n",
    "    labels = [0]*len1 + [1]*len2\n",
    "    dataset = tokenizer.batch_encode_plus(combined, add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs = dataset[\"input_ids\"]\n",
    "    attn_mask = dataset[\"attention_mask\"]\n",
    "    context_len = inputs.size(1)\n",
    "    return LMDataset(inputs, attn_mask, torch.tensor(labels)), context_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datatype, sub_datatype, classification, batch_size):\n",
    "\n",
    "    train_data_path = f\"{path}/data/{datatype}/train_data_{sub_datatype}.pkl\"\n",
    "    val_data_path = f\"{path}/data/{datatype}/valid_data_{sub_datatype}.pkl\"\n",
    "\n",
    "    with open(train_data_path, \"rb\") as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(val_data_path, \"rb\") as f:\n",
    "        val_data = pickle.load(f)\n",
    "\n",
    "    if sub_datatype == \"natural\":\n",
    "        train_data = train_data\n",
    "        val_data = val_data\n",
    "        train_labels = train_data\n",
    "        val_labels = val_data\n",
    "    else:\n",
    "        if classification:\n",
    "            train_labels = train_data[\"labels\"]\n",
    "            val_labels = val_data[\"labels\"]\n",
    "        else:\n",
    "            train_labels = train_data[\"inputs\"]\n",
    "            val_labels = val_data[\"inputs\"]\n",
    "        \n",
    "        train_data = train_data[\"inputs\"]\n",
    "        val_data = val_data[\"inputs\"]\n",
    "\n",
    "    trainset = LMDataset(train_data, labels=train_labels)\n",
    "    valset = LMDataset(val_data, labels=val_labels)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(embedding_dim, n_layer, n_head, resid_pdrop, embd_pdrop, attn_pdrop, classification, num_labels = None):\n",
    "    model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = 128,\n",
    "                            resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels)\n",
    "    if classification:\n",
    "        model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "    else:\n",
    "        model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in tqdm(enumerate(valloader), total=len(valloader)):\n",
    "        with torch.no_grad():\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            if \"attention_mask\" in batch:\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(inputs, labels=labels, attention_mask=attention_mask)\n",
    "        losses.append(outputs.loss)\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader):\n",
    "    wandb.init(project=\"dimensionality reduction\", config=train_config)\n",
    "    run_name = wandb.run.name\n",
    "\n",
    "    save_path = f\"{path}/models/{model_name}/{run_name}\"\n",
    "    save_epochs = train_config[\"num_epochs\"] // 10\n",
    "    save_epochs = 1\n",
    "\n",
    "    for epoch in tqdm(range(train_config[\"num_epochs\"])):\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        for step, batch in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            if \"attention_mask\" in batch:\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(inputs, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss \n",
    "            loss.backward()\n",
    "            if train_config[\"lr_scheduler\"] is not None:\n",
    "                lr_scheduler.step()\n",
    "            optimizer.step()\n",
    "\n",
    "            wandb.log({\"step\": step + len(trainloader) * epoch})\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        val_loss = evaluate(model, valloader)\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "\n",
    "        #save model\n",
    "        if epoch % save_epochs == 0:\n",
    "            model_dir = os.path.join(save_path, f\"epoch_{epoch}\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.save_pretrained(model_dir)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #save model\n",
    "    model_dir = os.path.join(save_path, \"final_chkpoint\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model.save_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240709_221145-ygjcm3ro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/ygjcm3ro' target=\"_blank\">happy-microwave-36</a></strong> to <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/ygjcm3ro' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/ygjcm3ro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "classification = False\n",
    "num_labels = 2\n",
    "\n",
    "datatype = \"natural_language\"\n",
    "sub_datatype = \"natural\"\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 768\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "\n",
    "model = load_model(embedding_dim, n_layer, n_head, resid_pdrop, embd_pdrop, attn_pdrop, classification, num_labels)\n",
    "model.to(device)\n",
    "trainloader, valloader = load_data(datatype, sub_datatype, classification, batch_size)\n",
    "if not classification:\n",
    "    model_type = \"lm\"\n",
    "else:\n",
    "    model_type = \"class\"\n",
    "\n",
    "lr_scheduler = None\n",
    "model_name = f\"{datatype}/{sub_datatype}/{model_type}\"\n",
    "train_config = {\"num_epochs\": 20, \"lr\": 0.00005, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"datatype\": datatype, \"sub_datatype\": sub_datatype, \"num_labels\": num_labels}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240620_162105-cb2aymac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/cb2aymac' target=\"_blank\">sunny-feather-23</a></strong> to <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/cb2aymac' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/cb2aymac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.54it/s]\n",
      "100%|██████████| 157/157 [02:19<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.54it/s]\n",
      "100%|██████████| 157/157 [02:19<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.54it/s]\n",
      "100%|██████████| 157/157 [02:19<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:19<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:19<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:19<00:00,  1.13it/s]t]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      " 60%|█████▉    | 94/157 [01:24<00:56,  1.12it/s]s/it]\n",
      " 45%|████▌     | 45/100 [1:54:13<2:19:36, 152.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m         lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, labels\u001b[38;5;241m=\u001b[39mlabels, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classification = False\n",
    "num_labels = 2\n",
    "\n",
    "datatype = \"natural_language\"\n",
    "sub_datatype = \"fourgram_B\"\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 768\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "\n",
    "model = load_model(embedding_dim, n_layer, n_head, resid_pdrop, embd_pdrop, attn_pdrop, classification, num_labels)\n",
    "model.to(device)\n",
    "trainloader, valloader = load_data(datatype, sub_datatype, classification, batch_size)\n",
    "if not classification:\n",
    "    model_type = \"lm\"\n",
    "else:\n",
    "    model_type = \"class\"\n",
    "\n",
    "lr_scheduler = None\n",
    "model_name = f\"{datatype}/{sub_datatype}/{model_type}\"\n",
    "train_config = {\"num_epochs\": 100, \"lr\": 0.00005, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"datatype\": datatype, \"sub_datatype\": sub_datatype, \"num_labels\": num_labels}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cb2aymac) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>44</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>loss</td><td>2.18329</td></tr><tr><td>step</td><td>7158</td></tr><tr><td>val_loss</td><td>4.70727</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-feather-23</strong> at: <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/cb2aymac' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/cb2aymac</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240620_162105-cb2aymac/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cb2aymac). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240620_181539-6dze4hov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/6dze4hov' target=\"_blank\">usual-disco-24</a></strong> to <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/6dze4hov' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/6dze4hov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [02:17<00:00,  1.14it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:17<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]it]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]  \n",
      "100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.14it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 157/157 [02:18<00:00,  1.13it/s]]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.56it/s]\n",
      "100%|██████████| 50/50 [2:05:13<00:00, 150.27s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>loss</td><td>1.84485</td></tr><tr><td>step</td><td>7849</td></tr><tr><td>val_loss</td><td>4.10728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-disco-24</strong> at: <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/6dze4hov' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/6dze4hov</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240620_181539-6dze4hov/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8drkywnj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>loss</td><td>0.53211</td></tr><tr><td>step</td><td>5353</td></tr><tr><td>val_loss</td><td>0.53326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-elevator-28</strong> at: <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/8drkywnj' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/8drkywnj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240620_235331-8drkywnj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8drkywnj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240621_011916-5eogwt5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/5eogwt5b' target=\"_blank\">glowing-glade-29</a></strong> to <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/5eogwt5b' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/5eogwt5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 13/157 [00:11<02:07,  1.13it/s]\n",
      "  0%|          | 0/50 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m         lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)\u001b[0m\n\u001b[1;32m     18\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[1;32m    388\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 389\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m(hidden_states)\n\u001b[1;32m    390\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    391\u001b[0m         hidden_states,\n\u001b[1;32m    392\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1607\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m-> 1607\u001b[0m     _buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_buffers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _buffers:\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _buffers[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "classification = False\n",
    "num_labels = 2\n",
    "\n",
    "datatype = \"toy\"\n",
    "sub_datatype = \"fourgram_A\"\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 768\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "\n",
    "model = load_model(embedding_dim, n_layer, n_head, resid_pdrop, embd_pdrop, attn_pdrop, classification, num_labels)\n",
    "model.to(device)\n",
    "trainloader, valloader = load_data(datatype, sub_datatype, classification, batch_size)\n",
    "if not classification:\n",
    "    model_type = \"lm\"\n",
    "else:\n",
    "    model_type = \"class\"\n",
    "\n",
    "lr_scheduler = None\n",
    "model_name = f\"{datatype}/{sub_datatype}/{model_type}\"\n",
    "train_config = {\"num_epochs\": 50, \"lr\": 0.00001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"datatype\": datatype, \"sub_datatype\": sub_datatype, \"num_labels\": num_labels}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240620_220010-b5y4e3nm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/b5y4e3nm' target=\"_blank\">hardy-moon-27</a></strong> to <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/b5y4e3nm' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/b5y4e3nm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [01:55<00:00,  1.15it/s]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]/it]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      " 73%|███████▎  | 97/132 [01:27<00:31,  1.11it/s]s/it]\n",
      " 53%|█████▎    | 53/100 [1:53:07<1:40:18, 128.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m         lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, labels\u001b[38;5;241m=\u001b[39mlabels, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om2/user/jackking/anaconda/envs/modular_transformers/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classification = False\n",
    "num_labels = 2\n",
    "\n",
    "datatype = \"fragment\"\n",
    "sub_datatype = \"B\"\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 768\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "\n",
    "model = load_model(embedding_dim, n_layer, n_head, resid_pdrop, embd_pdrop, attn_pdrop, classification, num_labels)\n",
    "model.to(device)\n",
    "trainloader, valloader = load_data(datatype, sub_datatype, classification, batch_size)\n",
    "if not classification:\n",
    "    model_type = \"lm\"\n",
    "else:\n",
    "    model_type = \"class\"\n",
    "\n",
    "lr_scheduler = None\n",
    "model_name = f\"{datatype}/{sub_datatype}/{model_type}\"\n",
    "train_config = {\"num_epochs\": 100, \"lr\": 0.00005, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"datatype\": datatype, \"sub_datatype\": sub_datatype, \"num_labels\": num_labels}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:b5y4e3nm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>52</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>loss</td><td>0.7083</td></tr><tr><td>step</td><td>7092</td></tr><tr><td>val_loss</td><td>0.81086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-moon-27</strong> at: <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/b5y4e3nm' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/b5y4e3nm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240620_220010-b5y4e3nm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:b5y4e3nm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240620_235331-8drkywnj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/8drkywnj' target=\"_blank\">chocolate-elevator-28</a></strong> to <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/8drkywnj' target=\"_blank\">https://wandb.ai/modular_transformers/dimensionality%20reduction/runs/8drkywnj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:55<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.57it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 132/132 [01:56<00:00,  1.14it/s]t]\n",
      "100%|██████████| 33/33 [00:09<00:00,  3.59it/s]\n",
      " 22%|██▏       | 22/100 [46:25<2:44:24, 126.47s/it]"
     ]
    }
   ],
   "source": [
    "classification = False\n",
    "num_labels = 2\n",
    "\n",
    "datatype = \"cycle\"\n",
    "sub_datatype = \"B\"\n",
    "batch_size = 128\n",
    "\n",
    "embedding_dim = 768\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "\n",
    "model = load_model(embedding_dim, n_layer, n_head, resid_pdrop, embd_pdrop, attn_pdrop, classification, num_labels)\n",
    "model.to(device)\n",
    "trainloader, valloader = load_data(datatype, sub_datatype, classification, batch_size)\n",
    "if not classification:\n",
    "    model_type = \"lm\"\n",
    "else:\n",
    "    model_type = \"class\"\n",
    "\n",
    "lr_scheduler = None\n",
    "model_name = f\"{datatype}/{sub_datatype}/{model_type}\"\n",
    "train_config = {\"num_epochs\": 100, \"lr\": 0.00005, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"datatype\": datatype, \"sub_datatype\": sub_datatype, \"num_labels\": num_labels}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
