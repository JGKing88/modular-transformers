{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import torch as torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "path = \"/om2/user/jackking/modular_transformers/scripts/input_statistics/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, inputs, attn_mask=None, labels=None):\n",
    "        #cast to tensors if not already tensors\n",
    "        if not torch.is_tensor(inputs):\n",
    "            inputs = torch.tensor(inputs)\n",
    "        if not torch.is_tensor(labels):\n",
    "            labels = torch.tensor(labels)\n",
    "        if attn_mask is not None and not torch.is_tensor(attn_mask):\n",
    "            attn_mask = torch.tensor(attn_mask)\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        self.attn_mask = attn_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'attention_mask': self.attn_mask[idx]}\n",
    "        elif self.attn_mask is None:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        else:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'attention_mask': self.attn_mask[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        return item\n",
    "\n",
    "def make_autoregressive_dataset(data):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    dataset = tokenizer.batch_encode_plus(data, add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs = dataset[\"input_ids\"]\n",
    "    attn_mask = dataset[\"attention_mask\"]\n",
    "    labels = dataset[\"input_ids\"].clone()\n",
    "    context_len = inputs.size(1)\n",
    "    return LMDataset(inputs, attn_mask, labels), context_len\n",
    "\n",
    "def make_classification_dataset(data1, data2):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    len1 = len(data1)\n",
    "    len2 = len(data2)\n",
    "    combined = data1 + data2\n",
    "    labels = [0]*len1 + [1]*len2\n",
    "    dataset = tokenizer.batch_encode_plus(combined, add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs = dataset[\"input_ids\"]\n",
    "    attn_mask = dataset[\"attention_mask\"]\n",
    "    context_len = inputs.size(1)\n",
    "    return LMDataset(inputs, attn_mask, torch.tensor(labels)), context_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_model_to_sample_from(data):\n",
    "    bigram_model = {}\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)-1):\n",
    "            if sentence[i] in bigram_model:\n",
    "                bigram_model[sentence[i]].append(sentence[i+1])\n",
    "            else:\n",
    "                bigram_model[sentence[i]] = [sentence[i+1]]\n",
    "\n",
    "    return bigram_model\n",
    "\n",
    "def sample_from_bigram_model(bigram_model, num_samples, string_len):\n",
    "    samples = []\n",
    "    bigram_model_keys = list(bigram_model.keys())\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        sample = [random.choice(bigram_model_keys)]\n",
    "        for _ in range(string_len - 1):\n",
    "            if sample[-1] not in bigram_model:\n",
    "                sample.append(random.choice(bigram_model_keys))\n",
    "            else:\n",
    "                sample.append(random.choice(bigram_model[sample[-1]]))\n",
    "        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note, with string_len=128 there are an insignifgant number of samples with a relevant attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_len = 128\n",
    "train_set_size = 20000\n",
    "valid_set_size = 5000\n",
    "datatype = \"experiment_1\"\n",
    "\n",
    "data_path = '/om/weka/evlab/ehoseini/MyData/miniBERTa_v2/'\n",
    "grouped_pad_train = load_from_disk(\n",
    "    os.path.join(data_path, f'miniBERTa-{10}M-crunched',\n",
    "                    f'train_context_len_{512}'))\n",
    "subset_idxs = np.random.choice(len(grouped_pad_train), train_set_size, replace=False)\n",
    "subset = grouped_pad_train.select(subset_idxs)[\"input_ids\"]\n",
    "subset = [x[:string_len] for x in subset]\n",
    "\n",
    "with open(f\"{path}/{datatype}/train_data_A.pkl\", 'wb') as f:\n",
    "    pickle.dump(subset, f)\n",
    "\n",
    "data_path = '/om/weka/evlab/ehoseini/MyData/miniBERTa_v2/'\n",
    "grouped_pad_train = load_from_disk(\n",
    "    os.path.join(data_path, f'miniBERTa-{10}M-crunched',\n",
    "                    f'valid_context_len_{512}'))\n",
    "subset_idxs = np.random.choice(len(grouped_pad_train), valid_set_size, replace=False)\n",
    "subset = grouped_pad_train.select(subset_idxs)[\"input_ids\"]\n",
    "subset = [x[:string_len] for x in subset]\n",
    "\n",
    "with open(f\"{path}/{datatype}/valid_data_A.pkl\", 'wb') as f:\n",
    "    pickle.dump(subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_len = 128\n",
    "train_set_size = 20000\n",
    "valid_set_size = 5000\n",
    "datatype = \"experiment_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data_A.pkl\", 'rb'))\n",
    "data = train_data + valid_data\n",
    "\n",
    "bigram_model = get_bigram_model_to_sample_from(data)\n",
    "data_B1 = sample_from_bigram_model(bigram_model, train_set_size//2 + valid_set_size//2, string_len = string_len)\n",
    "np.random.shuffle(data_B1)\n",
    "train_data_B1 = data_B1[:train_set_size//2]\n",
    "valid_data_B1 = data_B1[train_set_size//2:]\n",
    "\n",
    "#switch up bigram model\n",
    "new_bigram_model = {}\n",
    "\n",
    "all_tokens = bigram_model.keys()\n",
    "all_tokens = list(all_tokens)\n",
    "np.random.shuffle(all_tokens)\n",
    "\n",
    "for i, first_token in tqdm(enumerate(bigram_model.keys())):\n",
    "    second_tokens = bigram_model[all_tokens[i]]\n",
    "    # new_bigram_model[first_token] = [np.random.choice(second_tokens) for _ in range(len(second_tokens))]\n",
    "    new_bigram_model[first_token] = second_tokens\n",
    "\n",
    "data_B2 = sample_from_bigram_model(new_bigram_model, train_set_size//2 + valid_set_size//2, string_len = string_len)\n",
    "np.random.shuffle(data_B2)\n",
    "train_data_B2 = data_B2[:train_set_size//2]\n",
    "valid_data_B2 = data_B2[train_set_size//2:]\n",
    "\n",
    "# with open(f\"{path}/{datatype}/train_data_EB1.pkl\", 'wb') as f:\n",
    "#     pickle.dump(train_data_B1, f)\n",
    "\n",
    "# with open(f\"{path}/{datatype}/valid_data_EB1.pkl\", 'wb') as f:\n",
    "#     pickle.dump(valid_data_B1, f)\n",
    "\n",
    "with open(f\"{path}/{datatype}/train_data_EB2.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data_B2, f)\n",
    "\n",
    "with open(f\"{path}/{datatype}/valid_data_EB2.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data_B2, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_model_to_sample_from(data):\n",
    "    trigram_model = {}\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence) - 2):\n",
    "            first_two = (sentence[i], sentence[i+1])\n",
    "            if first_two in trigram_model:\n",
    "                trigram_model[first_two].append(sentence[i+2])\n",
    "            else:\n",
    "                trigram_model[first_two] = [sentence[i+2]]\n",
    "    \n",
    "    return trigram_model\n",
    "\n",
    "def sample_from_trigram_model(trigram_model, num_samples, string_len):\n",
    "    samples = []\n",
    "    trigram_model_keys = list(trigram_model.keys())\n",
    "    for _ in tqdm(range(num_samples)):\n",
    "        next_bigram = random.choice(trigram_model_keys)\n",
    "        sample = [next_bigram[0], next_bigram[1]]\n",
    "        for i in range(string_len - 2):\n",
    "            first_two = (sample[-2], sample[-1])\n",
    "            if first_two not in trigram_model:\n",
    "                next_bigram = random.choice(trigram_model_keys)\n",
    "                sample.append(next_bigram[0])\n",
    "                sample.append(next_bigram[1])\n",
    "                i += 1\n",
    "            else:\n",
    "                sample.append(random.choice(trigram_model[first_two]))\n",
    "        \n",
    "        if len(sample) > string_len:\n",
    "            sample = sample[:string_len]\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data_A.pkl\", 'rb'))\n",
    "data = train_data + valid_data\n",
    "\n",
    "trigram_model = get_trigram_model_to_sample_from(data)\n",
    "data_T1 = sample_from_trigram_model(trigram_model, train_set_size//2 + valid_set_size//2, string_len)\n",
    "#split into train and test randomly\n",
    "np.random.shuffle(data_T1)\n",
    "train_data_T1 = data_T1[:train_set_size//2]\n",
    "valid_data_T1 = data_T1[train_set_size//2:]\n",
    "\n",
    "with open(f\"{path}/{datatype}/train_data_ET1.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data_T1, f)\n",
    "\n",
    "with open(f\"{path}/{datatype}/valid_data_ET1.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data_T1, f)\n",
    "\n",
    "# #switch up bigram model\n",
    "new_trigram_model = {}\n",
    "\n",
    "all_tokens = trigram_model.keys()\n",
    "all_tokens = list(all_tokens)\n",
    "np.random.shuffle(all_tokens)\n",
    "\n",
    "for i, first_two_tokens in tqdm(enumerate(trigram_model.keys())):\n",
    "    third_tokens = trigram_model[all_tokens[i]]\n",
    "    # new_trigram_model[first_two_tokens] = [np.random.choice(third_tokens) for _ in range(len(third_tokens))]\n",
    "    new_trigram_model[first_two_tokens] = third_tokens\n",
    "\n",
    "data_T2 = sample_from_trigram_model(new_trigram_model, train_set_size//2 + valid_set_size//2, string_len)\n",
    "#split into train and test randomly\n",
    "np.random.shuffle(data_T2)\n",
    "train_data_T2 = data_T2[:train_set_size//2]\n",
    "valid_data_T2 = data_T2[train_set_size//2:]\n",
    "\n",
    "with open(f\"{path}/{datatype}/train_data_ET2.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data_T2, f)\n",
    "\n",
    "with open(f\"{path}/{datatype}/valid_data_ET2.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data_T2, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourgram_model_to_sample_from(data):\n",
    "    fourgram_model = {}\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence) - 3):\n",
    "            first_three = (sentence[i], sentence[i+1], sentence[i+2])\n",
    "            if first_three in fourgram_model:\n",
    "                fourgram_model[first_three].append(sentence[i+3])\n",
    "            else:\n",
    "                fourgram_model[first_three] = [sentence[i+3]]\n",
    "    \n",
    "    return fourgram_model\n",
    "\n",
    "def sample_from_fourgram_model(fourgram_model, num_samples, string_len):\n",
    "    samples = []\n",
    "    fourgram_model_keys = list(fourgram_model.keys())\n",
    "    for _ in tqdm(range(num_samples)):\n",
    "        next_trigram = random.choice(fourgram_model_keys)\n",
    "        sample = [next_trigram[0], next_trigram[1], next_trigram[2]]\n",
    "        for i in range(string_len - 3):\n",
    "            first_three = (sample[-3], sample[-2], sample[-1])\n",
    "            if first_three not in fourgram_model:\n",
    "                next_trigram = random.choice(fourgram_model_keys)\n",
    "                sample.append(next_trigram[0])\n",
    "                sample.append(next_trigram[1])\n",
    "                sample.append(next_trigram[2])\n",
    "                i += 2\n",
    "            else:\n",
    "                sample.append(random.choice(fourgram_model[first_three]))\n",
    "        if len(sample) > string_len:\n",
    "            sample = sample[:string_len]\n",
    "        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data_A.pkl\", 'rb'))\n",
    "data = train_data + valid_data\n",
    "\n",
    "fourgram_model = get_fourgram_model_to_sample_from(data)\n",
    "data_F1 = sample_from_fourgram_model(fourgram_model, train_set_size//2 + valid_set_size//2, string_len)\n",
    "np.random.shuffle(data_F1)\n",
    "train_data_F1 = data_F1[:train_set_size//2]\n",
    "valid_data_F1 = data_F1[train_set_size//2:]\n",
    "\n",
    "with open(f\"{path}/{datatype}/train_data_EF1.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data_F1, f)\n",
    "\n",
    "with open(f\"{path}/{datatype}/valid_data_EF1.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data_F1, f)\n",
    "\n",
    "#switch up bigram model\n",
    "new_fourgram_model = {}\n",
    "\n",
    "# for first_three_tokens, fourth_tokens in tqdm(fourgram_model.items()):\n",
    "#     fourth_tokens_set = list(set(fourth_tokens))\n",
    "#     new_fourgram_model[first_three_tokens] = [np.random.choice(fourth_tokens_set) for _ in range(len(fourth_tokens))]\n",
    "\n",
    "all_tokens = fourgram_model.keys()\n",
    "all_tokens = list(all_tokens)\n",
    "np.random.shuffle(all_tokens)\n",
    "\n",
    "for i, first_token in tqdm(enumerate(fourgram_model.keys())):\n",
    "    fourth_tokens = fourgram_model[all_tokens[i]]\n",
    "    # new_fourgram_model[first_token] = [np.random.choice(fourth_tokens) for _ in range(len(fourth_tokens))]\n",
    "    new_fourgram_model[first_token] = fourth_tokens\n",
    "\n",
    "data_F2 = sample_from_fourgram_model(new_fourgram_model, train_set_size//2 + valid_set_size//2, string_len)\n",
    "np.random.shuffle(data_F2)\n",
    "train_data_F2 = data_F2[:train_set_size//2]\n",
    "valid_data_F2 = data_F2[train_set_size//2:]\n",
    "\n",
    "with open(f\"{path}/{datatype}/train_data_EF2.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data_F2, f)\n",
    "\n",
    "with open(f\"{path}/{datatype}/valid_data_EF2.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data_F2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data_A.pkl\", 'rb'))\n",
    "data = train_data + valid_data\n",
    "\n",
    "fourgram_model = get_fourgram_model_to_sample_from(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourgram_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(fourgram_model[key]) for key in fourgram_model.keys()]\n",
    "#take top 20 lengths\n",
    "lengths = sorted(lengths, reverse=True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [(len(fourgram_model[key]), key) for key in fourgram_model.keys()]\n",
    "print(max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evening out Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(model, dataloader):\n",
    "    entropies = []\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        # Get model outputs (logits)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"].to(device))\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "\n",
    "        # Calculate entropy for each token\n",
    "        token_entropies = -torch.sum(probs * torch.log(probs), dim=-1)\n",
    "\n",
    "        # Average entropy over all tokens in the text\n",
    "        avg_entropy = token_entropies.mean().item()\n",
    "        entropies.append(avg_entropy)\n",
    "    \n",
    "    # Calculate average entropy over all texts in the dataset\n",
    "    dataset_entropy = np.mean(entropies)\n",
    "    return dataset_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"experiment_1/M1_128_12/faithful-elevator-344/epoch_80\"\n",
    "model_path = f'{path}/{model_name}'\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "datatype = \"experiment_1\"\n",
    "\n",
    "natural_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "dataset = LMDataset(natural_data, labels = natural_data)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "dataset_entropy = calculate_entropy(model, dataloader)\n",
    "print(f\"Dataset entropy: {dataset_entropy}\")\n",
    "\n",
    "bigram_data = pickle.load(open(f\"{path}/{datatype}/train_data_EB1.pkl\", 'rb')) + pickle.load(open(f\"{path}/{datatype}/train_data_EB2.pkl\", 'rb'))\n",
    "dataset = LMDataset(bigram_data, labels = natural_data)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "dataset_entropy = calculate_entropy(model, dataloader)\n",
    "print(f\"Bigram dataset entropy: {dataset_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_data = pickle.load(open(f\"{path}/{datatype}/train_data_EB1.pkl\", 'rb')) + pickle.load(open(f\"{path}/{datatype}/train_data_EB2.pkl\", 'rb'))\n",
    "dataset = LMDataset(bigram_data, labels = natural_data)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "dataset_entropy = calculate_entropy(model, dataloader)\n",
    "print(f\"Bigram dataset entropy: {dataset_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, FreqDist, ConditionalFreqDist\n",
    "import numpy as np\n",
    "\n",
    "# Ensure necessary NLTK data packages are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_bigram_entropy(texts):\n",
    "    # Create bigrams for all texts\n",
    "    bigrams_list = [bigram for text in texts for bigram in bigrams(text)]\n",
    "\n",
    "    # Frequency distribution of bigrams and individual words\n",
    "    bigram_fd = FreqDist(bigrams_list)\n",
    "    word_fd = FreqDist(word for text in texts for word in text)\n",
    "\n",
    "    # Conditional frequency distribution of bigrams\n",
    "    cfd = ConditionalFreqDist(bigrams_list)\n",
    "\n",
    "    # Calculate bigram probabilities\n",
    "    bigram_probabilities = {}\n",
    "    for word in word_fd:\n",
    "        for following_word in cfd[word]:\n",
    "            bigram_probabilities[(word, following_word)] = cfd[word][following_word] / word_fd[word]\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropies = []\n",
    "    for bigram, prob in bigram_probabilities.items():\n",
    "        entropies.append(-prob * np.log2(prob))\n",
    "\n",
    "    # Average entropy over all bigrams\n",
    "    dataset_entropy = np.mean(entropies)\n",
    "    return dataset_entropy\n",
    "\n",
    "bigram_data = pickle.load(open(f\"{path}/{datatype}/train_data_B1.pkl\", 'rb')) + pickle.load(open(f\"{path}/{datatype}/valid_data_B2.pkl\", 'rb'))\n",
    "calculate_bigram_entropy(bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_of_bigram_model(bigram_model, unigram_frequencies):\n",
    "    reverse_bigram_model = {}\n",
    "    for first_word, second_words in bigram_model.items():\n",
    "        for second_word in second_words:\n",
    "            if second_word in reverse_bigram_model:\n",
    "                reverse_bigram_model[second_word].append(first_word)\n",
    "            else:\n",
    "                reverse_bigram_model[second_word] = [first_word]\n",
    "    \n",
    "    entropy = 0\n",
    "    for second_word, first_words in tqdm(reverse_bigram_model.items()):\n",
    "        first_words = set(first_words)\n",
    "        probs_second_word = [bigram_model[first_word].count(second_word)/len(bigram_model[first_word]) * unigram_frequencies[first_word]/len(unigram_frequencies) for first_word in first_words]\n",
    "        prob_second_word = sum(probs_second_word)\n",
    "        entropy_second_word = -prob_second_word*np.log2(prob_second_word)\n",
    "        entropy += entropy_second_word\n",
    "    \n",
    "    return entropy / len(reverse_bigram_model)\n",
    "\n",
    "\n",
    "def get_bigram_model_with_correct_entropy(data, entropy_target):\n",
    "    bigram_model = {}\n",
    "    unigram_frequencies = {}\n",
    "    for sentence_idx, sentence in enumerate(data):\n",
    "\n",
    "        for i in range(len(sentence)-1):\n",
    "\n",
    "            if sentence[i] in unigram_frequencies:\n",
    "                unigram_frequencies[sentence[i]] += 1\n",
    "            else:\n",
    "                unigram_frequencies[sentence[i]] = 1\n",
    "\n",
    "            if sentence[i] in bigram_model:\n",
    "                bigram_model[sentence[i]].append(sentence[i+1])\n",
    "            else:\n",
    "                bigram_model[sentence[i]] = [sentence[i+1]]\n",
    "\n",
    "        # if sentence_idx > 15000 and sentence_idx % 200 == 0:\n",
    "        #     entropy = calculate_entropy_of_bigram_model(bigram_model, unigram_frequencies)\n",
    "        #     print(f\"Entropy: {entropy}\")\n",
    "        #     if entropy > entropy_target:\n",
    "        #         break\n",
    "\n",
    "    print(f\"Bigram model entropy: {calculate_entropy_of_bigram_model(bigram_model, unigram_frequencies)}\")\n",
    "    return bigram_model\n",
    "\n",
    "def sample_from_bigram_model(bigram_model, num_samples, string_len = None, lengths=None):\n",
    "    samples = []\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        sample = [np.random.choice(list(bigram_model.keys()))]\n",
    "        length = string_len\n",
    "        for _ in range(length - 1):\n",
    "            if sample[-1] not in bigram_model:\n",
    "                sample.append(np.random.choice(list(bigram_model.keys())))\n",
    "            else:\n",
    "                sample.append(np.random.choice(bigram_model[sample[-1]]))\n",
    "        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = get_bigram_model_with_correct_entropy(data, dataset_entropy)\n",
    "# train_data_B1 = sample_from_bigram_model(bigram_model, train_set_size//2, string_len = string_len)\n",
    "# valid_data_B1 = sample_from_bigram_model(bigram_model, valid_set_size//2, string_len = string_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_entropy_of_dataset(data, bigram_model):\n",
    "    reverse_bigram_model = {}\n",
    "    for first_word, second_words in bigram_model.items():\n",
    "        for second_word in second_words:\n",
    "            if second_word in reverse_bigram_model:\n",
    "                reverse_bigram_model[second_word].append(first_word)\n",
    "            else:\n",
    "                reverse_bigram_model[second_word] = [first_word]\n",
    "\n",
    "    new_reverse_bigram_model = {}\n",
    "    for second_word, first_words in reverse_bigram_model.items():\n",
    "        first_word_set = set(first_words)\n",
    "        new_reverse_bigram_model[second_word] = {}\n",
    "        for first_word in first_word_set:\n",
    "            count = first_words.count(first_word)\n",
    "            prob = count/len(first_words)\n",
    "            new_reverse_bigram_model[second_word][first_word] = prob\n",
    "\n",
    "    entropy = 0\n",
    "    for sentence in tqdm(data):\n",
    "        for i in range(1, len(sentence)):\n",
    "            try:\n",
    "                prob = new_reverse_bigram_model[sentence[i]][sentence[i-1]]\n",
    "                entropy += -prob*np.log2(prob)\n",
    "            except:\n",
    "                print(\"error\")\n",
    "\n",
    "    return entropy / (len(data) * len(data[0]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_data = pickle.load(open(f\"{path}/{datatype}/train_data_B1.pkl\", 'rb')) + pickle.load(open(f\"{path}/{datatype}/train_data_B2.pkl\", 'rb'))\n",
    "print(calculate_bigram_entropy_of_dataset(bigram_data, bigram_model))\n",
    "natural_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "print(calculate_bigram_entropy_of_dataset(natural_data, bigram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dimension and the variance of the perturbation\n",
    "dim = 3000\n",
    "sigma = 1\n",
    "num_samples = 20000\n",
    "\n",
    "# Define the original points in a high-dimensional space\n",
    "A = np.random.rand(dim)\n",
    "B = np.random.rand(dim)\n",
    "C = np.random.rand(dim)\n",
    "\n",
    "# Function to calculate the angle between two vectors\n",
    "def calculate_angle(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    cos_theta = dot_product / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    return theta\n",
    "\n",
    "# Generate random perturbations and calculate angles\n",
    "angles = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    P = np.random.normal(0, sigma, dim) \n",
    "    C_perturbed = C + P\n",
    "    angle = calculate_angle(A - B, C_perturbed - B)\n",
    "    angles.append(angle)\n",
    "\n",
    "# Calculate the average angle in radians and degrees\n",
    "average_angle_radians = np.mean(angles)\n",
    "average_angle_degrees = np.degrees(average_angle_radians)\n",
    "\n",
    "average_angle_radians, average_angle_degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out lines are used when there is a leave probability\n",
    "\n",
    "vocabulary = np.arange(1, 5000, 1)\n",
    "context_length = 10\n",
    "cycle_len = 10\n",
    "leave_probability = 0.03\n",
    "num_samples = 50000\n",
    "num_cycles = 200\n",
    "\n",
    "# total_probability = 1 - leave_probability\n",
    "total_probability = 1\n",
    "probability_model = np.exp(-np.arange(1, cycle_len, 1) / 5)\n",
    "probability_model = probability_model/np.sum(probability_model)\n",
    "probability_model = total_probability*probability_model\n",
    "# probability_model = np.concatenate((probability_model, [leave_probability]))\n",
    "token_picker = np.arange(1, cycle_len)\n",
    "# token_picker = np.concatenate((token_picker, [-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample cycles from vocabulary\n",
    "cycles = []\n",
    "for _ in range(num_cycles):\n",
    "    sample = np.random.choice(vocabulary, size=cycle_len, replace=False)\n",
    "    cycles.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "cycle_indexes = [] #labels\n",
    "for sample in tqdm(range(num_samples)):\n",
    "    sample = []\n",
    "\n",
    "    cycle_index = np.random.choice(num_cycles)\n",
    "    cycle_indexes.append(cycle_index)\n",
    "    cycle = cycles[cycle_index]\n",
    "\n",
    "    token_index = np.random.choice(cycle_len)\n",
    "    sample.append(cycle[token_index])\n",
    "\n",
    "    for i in range(context_length-1):\n",
    "        next_token_increment = np.random.choice(token_picker, p=probability_model)\n",
    "        if next_token_increment == -100:\n",
    "            cycle_index = np.random.choice(num_cycles)\n",
    "            cycle = cycles[cycle_index]\n",
    "            token_index = np.random.choice(cycle_len)\n",
    "        else:\n",
    "            token_index = int((token_index+next_token_increment) % cycle_len)\n",
    "\n",
    "        sample.append(cycle[token_index])\n",
    "    \n",
    "    samples.append(sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "i = 0\n",
    "for indx in range(1, len(cycle_indexes)):\n",
    "    if cycle_indexes[indx-1] == cycle_indexes[indx]:\n",
    "        i += 1\n",
    "    else:\n",
    "        lengths.append(i)\n",
    "        i = 0\n",
    "\n",
    "plt.hist(lengths, bins=50, alpha=0.5, label='generated data')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"{path}/experiment_2\"\n",
    "cutoff = int(num_samples*(4/5))\n",
    "train_data = {\"inputs\": samples[:cutoff], \"labels\": cycle_indexes[:cutoff]}\n",
    "valid_data = {\"inputs\": samples[cutoff:], \"labels\": cycle_indexes[cutoff:]}\n",
    "\n",
    "with open(f\"{data_path}/train_data_D.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open(f\"{data_path}/valid_data_D.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.arange(1, 500, 1)\n",
    "\n",
    "context_length = 128\n",
    "cycle_len = 24\n",
    "num_samples = 20000\n",
    "num_cycles = 200\n",
    "\n",
    "#sample cycles from vocabulary\n",
    "cycles = []\n",
    "cycle_probs = []\n",
    "for _ in range(num_cycles):\n",
    "    sample = np.random.choice(vocabulary, size=cycle_len, replace=False)\n",
    "    cycles.append(sample)\n",
    "    probs = np.random.uniform(0.65, 0.85, size=cycle_len)\n",
    "    cycle_probs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_integer(n, cycle_len):\n",
    "    pieces = []\n",
    "    low_end = int(cycle_len * (4/8))\n",
    "    high_end = int(cycle_len * (7/8))\n",
    "    while n > 0:\n",
    "        piece = random.randint(low_end, high_end)\n",
    "        if n - piece < 0:\n",
    "            piece = n\n",
    "        pieces.append(piece)\n",
    "        n -= piece\n",
    "    return pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2S\n",
    "\n",
    "samples = []\n",
    "cycle_indexes = [] #labels\n",
    "for sample_idx in tqdm(range(num_samples)):\n",
    "    sample = []\n",
    "\n",
    "    cycle_index = sample_idx % num_cycles\n",
    "    cycle_indexes.append(cycle_index)\n",
    "    cycle = cycles[cycle_index]\n",
    "\n",
    "    token_index = np.random.choice(cycle_len)\n",
    "    sample.append(cycle[token_index])\n",
    "\n",
    "    for i in range(context_length-1):\n",
    "        move_prob = cycle_probs[cycle_index][token_index]\n",
    "        if np.random.uniform() < move_prob:\n",
    "            next_token_increment = 1\n",
    "        else: \n",
    "            next_token_increment = 0\n",
    "\n",
    "        token_index = int((token_index+next_token_increment) % cycle_len)\n",
    "        sample.append(cycle[token_index])\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "cutoff = int(num_samples*(3/4))\n",
    "train_data = {\"inputs\": samples[:cutoff], \"labels\": cycle_indexes[:cutoff]}\n",
    "valid_data = {\"inputs\": samples[cutoff:], \"labels\": cycle_indexes[cutoff:]}\n",
    "\n",
    "with open(f\"{data_path}/train_data_C.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open(f\"{data_path}/valid_data_C.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2F\n",
    "\n",
    "samples = []\n",
    "cycle_indexes = [] #labels\n",
    "for sample_idx in tqdm(range(num_samples)):\n",
    "    sample = []\n",
    "\n",
    "    cycle_index = sample_idx % num_cycles\n",
    "    cycle_indexes.append(cycle_index)\n",
    "    cycle = cycles[cycle_index]\n",
    "    fragment_sizes = break_integer(context_length, cycle_len)\n",
    "\n",
    "    for size in fragment_sizes:\n",
    "\n",
    "        token_index = random.randint(0, cycle_len-size-1)\n",
    "        sample.append(cycle[token_index])\n",
    "\n",
    "        for i in range(size-1):\n",
    "            move_prob = cycle_probs[cycle_index][token_index]\n",
    "            if np.random.uniform() < move_prob:\n",
    "                next_token_increment = 1\n",
    "            else: \n",
    "                next_token_increment = 0\n",
    "\n",
    "            token_index = int(token_index+next_token_increment)\n",
    "            sample.append(cycle[token_index])\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "data_path = f\"{path}/experiment_2F\"\n",
    "cutoff = int(num_samples*(4/5))\n",
    "train_data = {\"inputs\": samples[:cutoff], \"labels\": cycle_indexes[:cutoff]}\n",
    "valid_data = {\"inputs\": samples[cutoff:], \"labels\": cycle_indexes[cutoff:]}\n",
    "\n",
    "with open(f\"{data_path}/train_data_G.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open(f\"{data_path}/valid_data_G.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.arange(1, 43748, 1)\n",
    "prob_model = np.exp(-np.arange(1, 43748, 1)/43748 * 100)\n",
    "prob_model = prob_model/np.sum(prob_model)\n",
    "\n",
    "context_length = 128\n",
    "num_samples = 20000\n",
    "\n",
    "samples = []\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    sample = np.random.choice(vocabulary, size=context_length, p=prob_model, replace=True)\n",
    "    samples.append(sample)\n",
    "\n",
    "data_path = f\"{path}/random\"\n",
    "cutoff = int(num_samples*(3/4))\n",
    "train_data = samples[:cutoff]\n",
    "valid_data = samples[cutoff:]\n",
    "\n",
    "with open(f\"{data_path}/train_data.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open(f\"{data_path}/valid_data.pkl\", 'wb') as f:\n",
    "    pickle.dump(valid_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "import torch as torch\n",
    "import wandb\n",
    "from modular_transformers.straightening.straightening_utils import compute_model_activations, compute_model_curvature\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "path = \"/om2/user/jackking/modular_transformers/scripts/input_statistics\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curvature(model_name, data):\n",
    "    if model_name == \"untrained\":\n",
    "        embedding_dim = 128\n",
    "        n_layer = 12                       \n",
    "        model_config = GPT2Config(n_layer = n_layer, n_head = 4, n_embd = embedding_dim, ctx_len = ctx_len)\n",
    "        model = GPT2LMHeadModel._from_config(model_config)\n",
    "    elif model_name == \"gpt2\":\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    else:\n",
    "        model_path = f'{path}/{model_name}'\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "    model.to(device)\n",
    "    activations = compute_model_activations(model, data, device)\n",
    "    curvature = compute_model_curvature(activations)\n",
    "    curve = 180 / np.pi * curvature[\"curve\"]\n",
    "    return np.nanmean(curve, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"experiment_1/M2_B1_128_12/soft-plant-402/epoch_12\"\n",
    "val_data_B = pickle.load(open(f\"{path}/experiment_1/valid_data_B1.pkl\", 'rb'))\n",
    "curve = get_curvature(name, val_data_B)\n",
    "plt.plot(curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
