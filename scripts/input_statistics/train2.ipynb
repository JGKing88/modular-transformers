{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjack-g-king\u001b[0m (\u001b[33mmodular_transformers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jackking/.netrc\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, BatchEncoding, GPT2LMHeadModel, GPT2Config, GPT2ForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch as torch\n",
    "#from modular_transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from modular_transformers.models import components\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "from modular_transformers.straightening.straightening_utils import compute_model_activations, compute_model_curvature\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "path = \"/om2/user/jackking/modular_transformers/scripts/input_statistics/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wandb.login(key=\"a338f755915cccd861b14f29bf68601d8e1ec2c9\")\n",
    "\n",
    "#set seed\n",
    "seed = 21\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, inputs, attn_mask=None, labels=None):\n",
    "        #cast to tensors if not already tensors\n",
    "        if not torch.is_tensor(inputs):\n",
    "            inputs = torch.tensor(inputs)\n",
    "        if not torch.is_tensor(labels):\n",
    "            labels = torch.tensor(labels)\n",
    "        if attn_mask is not None and not torch.is_tensor(attn_mask):\n",
    "            attn_mask = torch.tensor(attn_mask)\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        self.attn_mask = attn_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'attention_mask': self.attn_mask[idx]}\n",
    "        elif self.attn_mask is None:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        else:\n",
    "            item = {\n",
    "                'input_ids': self.inputs[idx],\n",
    "                'attention_mask': self.attn_mask[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        return item\n",
    "\n",
    "def make_autoregressive_dataset(data):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    dataset = tokenizer.batch_encode_plus(data, add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs = dataset[\"input_ids\"]\n",
    "    attn_mask = dataset[\"attention_mask\"]\n",
    "    labels = dataset[\"input_ids\"].clone()\n",
    "    context_len = inputs.size(1)\n",
    "    return LMDataset(inputs, attn_mask, labels), context_len\n",
    "\n",
    "def make_classification_dataset(data1, data2):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    len1 = len(data1)\n",
    "    len2 = len(data2)\n",
    "    combined = data1 + data2\n",
    "    labels = [0]*len1 + [1]*len2\n",
    "    dataset = tokenizer.batch_encode_plus(combined, add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs = dataset[\"input_ids\"]\n",
    "    attn_mask = dataset[\"attention_mask\"]\n",
    "    context_len = inputs.size(1)\n",
    "    return LMDataset(inputs, attn_mask, torch.tensor(labels)), context_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(data):\n",
    "    blah = []\n",
    "    for sent in data:\n",
    "        blah.extend(sent)\n",
    "    vocab_size = len(set(blah))\n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"experiment_1\"\n",
    "\n",
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data_A.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data_A.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+valid_data)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_data)\n",
    "valset = LMDataset(valid_data, labels=valid_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "datatype = \"experiment_1\"\n",
    "train_data_B1 = pickle.load(open(f\"{path}/{datatype}/train_data_B1.pkl\", \"rb\"))\n",
    "val_data_B1 = pickle.load(open(f\"{path}/{datatype}/valid_data_B1.pkl\", \"rb\"))\n",
    "train_data_B2 = pickle.load(open(f\"{path}/{datatype}/train_data_B2.pkl\", \"rb\"))\n",
    "val_data_B2 = pickle.load(open(f\"{path}/{datatype}/valid_data_B2.pkl\", \"rb\"))\n",
    "\n",
    "train_data = train_data_B1 #+ train_data_B2\n",
    "val_data = val_data_B1 #+ val_data_B2\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_data)\n",
    "valset = LMDataset(val_data, labels=val_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M2_B1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "datatype = \"shorter_data\"\n",
    "train_data_T1 = pickle.load(open(f\"{path}/{datatype}/train_data_T1.pkl\", \"rb\"))\n",
    "val_data_T1 = pickle.load(open(f\"{path}/{datatype}/valid_data_T1.pkl\", \"rb\"))\n",
    "train_data_T2 = pickle.load(open(f\"{path}/{datatype}/train_data_T2.pkl\", \"rb\"))\n",
    "val_data_T2 = pickle.load(open(f\"{path}/{datatype}/valid_data_T2.pkl\", \"rb\"))\n",
    "\n",
    "train_data = train_data_T1 + train_data_T2\n",
    "val_data = val_data_T1 + val_data_T2\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_data)\n",
    "valset = LMDataset(val_data, labels=val_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M2_T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "datatype = \"shorter_data\"\n",
    "train_data_T1 = pickle.load(open(f\"{path}/{datatype}/train_data_F1.pkl\", \"rb\"))\n",
    "val_data_T1 = pickle.load(open(f\"{path}/{datatype}/valid_data_F1.pkl\", \"rb\"))\n",
    "train_data_T2 = pickle.load(open(f\"{path}/{datatype}/train_data_F2.pkl\", \"rb\"))\n",
    "val_data_T2 = pickle.load(open(f\"{path}/{datatype}/valid_data_F2.pkl\", \"rb\"))\n",
    "\n",
    "train_data = train_data_T1 + train_data_T2\n",
    "val_data = val_data_T1 + val_data_T2\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_data)\n",
    "valset = LMDataset(val_data, labels=val_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M2_F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "datatype = \"experiment_1\"\n",
    "train_data_B1 = pickle.load(open(f\"{path}/{datatype}/train_data_B1.pkl\", \"rb\"))\n",
    "val_data_B1 = pickle.load(open(f\"{path}/{datatype}/valid_data_B1.pkl\", \"rb\"))\n",
    "train_data_B2 = pickle.load(open(f\"{path}/{datatype}/train_data_B2.pkl\", \"rb\"))\n",
    "val_data_B2 = pickle.load(open(f\"{path}/{datatype}/valid_data_B2.pkl\", \"rb\"))\n",
    "\n",
    "len1 = len(train_data_B1)\n",
    "len2 = len(train_data_B2)\n",
    "train_data = train_data_B1 + train_data_B2\n",
    "train_labels = [0]*len1 + [1]*len2\n",
    "val_data = val_data_B1 + val_data_B2\n",
    "len1 = len(val_data_B1)\n",
    "len2 = len(val_data_B2)\n",
    "val_labels = [0]*len1 + [1]*len2\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "\n",
    "trainset = LMDataset(train_data, labels=torch.tensor(train_labels))\n",
    "valset = LMDataset(val_data, labels=torch.tensor(val_labels))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model_type = \"M3_B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "datatype = \"experiment_1\"\n",
    "train_data_T1 = pickle.load(open(f\"{path}/{datatype}/train_data_T1.pkl\", \"rb\"))\n",
    "val_data_T1 = pickle.load(open(f\"{path}/{datatype}/valid_data_T1.pkl\", \"rb\"))\n",
    "train_data_T2 = pickle.load(open(f\"{path}/{datatype}/train_data_T2.pkl\", \"rb\"))\n",
    "val_data_T2 = pickle.load(open(f\"{path}/{datatype}/valid_data_T2.pkl\", \"rb\"))\n",
    "\n",
    "len1 = len(train_data_T1)\n",
    "len2 = len(train_data_T2)\n",
    "train_data = train_data_T1 + train_data_T2\n",
    "train_labels = [0]*len1 + [1]*len2\n",
    "val_data = val_data_T1 + val_data_T2\n",
    "len1 = len(val_data_T1)\n",
    "len2 = len(val_data_T2)\n",
    "val_labels = [0]*len1 + [1]*len2\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "\n",
    "trainset = LMDataset(train_data, labels=torch.tensor(train_labels))\n",
    "valset = LMDataset(val_data, labels=torch.tensor(val_labels))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model_type = \"M3_T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "datatype = \"experiment_1\"\n",
    "train_data_T1 = pickle.load(open(f\"{path}/{datatype}/train_data_F1.pkl\", \"rb\"))\n",
    "val_data_T1 = pickle.load(open(f\"{path}/{datatype}/valid_data_F1.pkl\", \"rb\"))\n",
    "train_data_T2 = pickle.load(open(f\"{path}/{datatype}/train_data_F2.pkl\", \"rb\"))\n",
    "val_data_T2 = pickle.load(open(f\"{path}/{datatype}/valid_data_F2.pkl\", \"rb\"))\n",
    "\n",
    "len1 = len(train_data_T1)\n",
    "len2 = len(train_data_T2)\n",
    "train_data = train_data_T1 + train_data_T2\n",
    "train_labels = [0]*len1 + [1]*len2\n",
    "val_data = val_data_T1 + val_data_T2\n",
    "len1 = len(val_data_T1)\n",
    "len2 = len(val_data_T2)\n",
    "val_labels = [0]*len1 + [1]*len2\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "\n",
    "trainset = LMDataset(train_data, labels=torch.tensor(train_labels))\n",
    "valset = LMDataset(val_data, labels=torch.tensor(val_labels))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model_type = \"M3_F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"random\"\n",
    "\n",
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+valid_data)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_data)\n",
    "valset = LMDataset(valid_data, labels=valid_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"R1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 30, \"lr\": 0.0002, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"random\"\n",
    "\n",
    "train_data = pickle.load(open(f\"{path}/{datatype}/train_data.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/{datatype}/valid_data.pkl\", 'rb'))\n",
    "\n",
    "train_labels = np.random.randint(0, 2, len(train_data))\n",
    "val_labels = np.random.randint(0, 2, len(valid_data))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+valid_data)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_labels)\n",
    "valset = LMDataset(valid_data, labels=val_labels)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"R2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.3\n",
    "attn_pdrop = 0.3\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "# model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 50, \"lr\": 0.000001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2812766/2081996763.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "datatype = \"random\"\n",
    "\n",
    "train_data = pickle.load(open(f\"{path}/experiment_1/train_data_A.pkl\", 'rb'))\n",
    "train_data = train_data[:15000]\n",
    "valid_data = pickle.load(open(f\"{path}/experiment_1/valid_data_A.pkl\", 'rb'))\n",
    "valid_data = valid_data[:5000]\n",
    "\n",
    "train_labels = pickle.load(open(f\"{path}/{datatype}/train_data.pkl\", 'rb'))\n",
    "val_labels = pickle.load(open(f\"{path}/{datatype}/valid_data.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+valid_data)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_labels)\n",
    "valset = LMDataset(valid_data, labels=val_labels)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"X1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240612_144416-jederizx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/jederizx' target=\"_blank\">tough-firefly-442</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/jederizx' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/jederizx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.15it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.41it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.35it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.34it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.26it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.29it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.28it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.29it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.26it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.13it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.24it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.29it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.23it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.19it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.26it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.25it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.27it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.19it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.23it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.25it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.23it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.27it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.23it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.23it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.25it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.20it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.21it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.21it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.20it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.19it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.20it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.19it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.19it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.22it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.15it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.24it/s]\n",
      "100%|██████████| 50/50 [15:00<00:00, 18.01s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>learning_rate</td><td>0.0002</td></tr><tr><td>loss</td><td>6.54649</td></tr><tr><td>step</td><td>5899</td></tr><tr><td>val_loss</td><td>7.62655</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-firefly-442</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/jederizx' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/jederizx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240612_144416-jederizx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 50, \"lr\": 0.0002, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"experiment_1\"\n",
    "train_data_B1 = pickle.load(open(f\"{path}/{datatype}/train_data_B1.pkl\", \"rb\"))\n",
    "val_data_B1 = pickle.load(open(f\"{path}/{datatype}/valid_data_B1.pkl\", \"rb\"))\n",
    "train_data_B2 = pickle.load(open(f\"{path}/{datatype}/train_data_B2.pkl\", \"rb\"))\n",
    "val_data_B2 = pickle.load(open(f\"{path}/{datatype}/valid_data_B2.pkl\", \"rb\"))\n",
    "\n",
    "train_data = train_data_B1 + train_data_B2\n",
    "val_data = val_data_B1 + val_data_B2\n",
    "\n",
    "datatype = \"random\"\n",
    "\n",
    "train_labels = pickle.load(open(f\"{path}/{datatype}/train_data.pkl\", 'rb'))\n",
    "val_labels = pickle.load(open(f\"{path}/{datatype}/valid_data.pkl\", 'rb'))\n",
    "\n",
    "train_data = train_data[:15000]\n",
    "valid_data = val_data[:5000]\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_labels)\n",
    "valset = LMDataset(valid_data, labels=val_labels)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"X1_B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:50rpuanc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▄▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>7.08353</td></tr><tr><td>step</td><td>757</td></tr><tr><td>val_loss</td><td>7.09314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-salad-431</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/50rpuanc' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/50rpuanc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240612_104946-50rpuanc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:50rpuanc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240612_105205-v25ve0ap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/v25ve0ap' target=\"_blank\">treasured-pond-432</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/v25ve0ap' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/v25ve0ap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:15<00:00,  7.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.53it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.49it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.48it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.44it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.39it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.44it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.45it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.18it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.44it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.41it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.39it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.38it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.37it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.40it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.36it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.35it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.39it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.26it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.34it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.33it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.32it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.33it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.32it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.33it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.32it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.32it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.31it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.19it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 22.97it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.29it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.26it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.29it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.16it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.25it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.16it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.29it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.31it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.31it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.32it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.30it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.28it/s]\n",
      "100%|██████████| 40/40 [11:54<00:00, 17.85s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>████████▇▇█▇▇▇▇▇▇▇▇▇▆▇▆▆▅▅▅▄▅▄▃▄▃▃▂▂▂▂▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▆▅▅▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>6.90235</td></tr><tr><td>step</td><td>4719</td></tr><tr><td>val_loss</td><td>7.34712</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-pond-432</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/v25ve0ap' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/v25ve0ap</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240612_105205-v25ve0ap/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 40, \"lr\": 0.001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"random\"\n",
    "\n",
    "train_data = pickle.load(open(f\"{path}/experiment_1/train_data_A.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{path}/experiment_1/valid_data_A.pkl\", 'rb'))\n",
    "\n",
    "train_labels = np.random.randint(0, 2, len(train_data))\n",
    "val_labels = np.random.randint(0, 2, len(valid_data))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+valid_data)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_labels)\n",
    "valset = LMDataset(valid_data, labels=val_labels)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"X2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.3\n",
    "attn_pdrop = 0.3\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "# model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 50, \"lr\": 0.000001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"experiment_1\"\n",
    "train_data_B1 = pickle.load(open(f\"{path}/{datatype}/train_data_B1.pkl\", \"rb\"))\n",
    "val_data_B1 = pickle.load(open(f\"{path}/{datatype}/valid_data_B1.pkl\", \"rb\"))\n",
    "train_data_B2 = pickle.load(open(f\"{path}/{datatype}/train_data_B2.pkl\", \"rb\"))\n",
    "val_data_B2 = pickle.load(open(f\"{path}/{datatype}/valid_data_B2.pkl\", \"rb\"))\n",
    "\n",
    "train_data = train_data_B1 + train_data_B2\n",
    "val_data = val_data_B1 + val_data_B2\n",
    "\n",
    "datatype = \"random\"\n",
    "\n",
    "train_labels = np.random.randint(0, 2, len(train_data))\n",
    "val_labels = np.random.randint(0, 2, len(val_data))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "trainset = LMDataset(train_data, labels=train_labels)\n",
    "valset = LMDataset(valid_data, labels=val_labels)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"X2_B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.3\n",
    "attn_pdrop = 0.3\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "# model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 150, \"lr\": 0.000001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacategory = \"G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = f\"experiment_2S_{datacategory}\"\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = pickle.load(open(f\"{data_path}/train_data_{datacategory}.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{data_path}/valid_data_{datacategory}.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data[\"inputs\"]+valid_data[\"inputs\"])\n",
    "\n",
    "ctx_len = len(train_data[\"inputs\"][0])\n",
    "\n",
    "trainset = LMDataset(train_data[\"inputs\"], labels=train_data[\"inputs\"])\n",
    "valset = LMDataset(valid_data[\"inputs\"], labels=valid_data[\"inputs\"])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = f\"experiment_2S_{datacategory}\"\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "\n",
    "train_data = pickle.load(open(f\"{data_path}/train_data_{datacategory}.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{data_path}/valid_data_{datacategory}.pkl\", 'rb'))\n",
    "\n",
    "ctx_len = len(train_data[\"inputs\"][0])\n",
    "\n",
    "vocab_size = get_vocab_size(train_data[\"inputs\"] + valid_data[\"inputs\"])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_labels = 200\n",
    "\n",
    "trainset = LMDataset(train_data[\"inputs\"], labels=train_data[\"labels\"])\n",
    "valset = LMDataset(valid_data[\"inputs\"], labels=valid_data[\"labels\"])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in tqdm(enumerate(valloader), total=len(valloader)):\n",
    "        with torch.no_grad():\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            if \"attention_mask\" in batch:\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(inputs, labels=labels, attention_mask=attention_mask)\n",
    "        losses.append(outputs.loss)\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader):\n",
    "    wandb.init(project=\"input statistics\", config=train_config)\n",
    "    run_name = wandb.run.name\n",
    "\n",
    "    save_epochs = train_config[\"num_epochs\"] // 10\n",
    "\n",
    "    for epoch in tqdm(range(train_config[\"num_epochs\"])):\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        for step, batch in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            if \"attention_mask\" in batch:\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # print(inputs[0], labels[0])\n",
    "\n",
    "            outputs = model(inputs, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss \n",
    "            loss.backward()\n",
    "            if train_config[\"lr_scheduler\"] is not None:\n",
    "                lr_scheduler.step()\n",
    "            optimizer.step()\n",
    "\n",
    "            wandb.log({\"step\": step + len(trainloader) * epoch})\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        val_loss = evaluate(model, valloader)\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "\n",
    "        #save model\n",
    "        if epoch % save_epochs == 0:\n",
    "            model_dir = os.path.join(path, model_name, run_name, f\"epoch_{epoch}\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.save_pretrained(model_dir)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #save model\n",
    "    model_dir = os.path.join(path, model_name, run_name, \"final_chkpoint\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model.save_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacategory = \"C\"\n",
    "\n",
    "datatype = f\"experiment_2S_{datacategory}\"\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = pickle.load(open(f\"{data_path}/train_data_{datacategory}.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{data_path}/valid_data_{datacategory}.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data[\"inputs\"]+valid_data[\"inputs\"])\n",
    "\n",
    "ctx_len = len(train_data[\"inputs\"][0])\n",
    "\n",
    "trainset = LMDataset(train_data[\"inputs\"], labels=train_data[\"inputs\"])\n",
    "valset = LMDataset(valid_data[\"inputs\"], labels=valid_data[\"inputs\"])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240613_163614-6u8mo3rm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/6u8mo3rm' target=\"_blank\">wise-glitter-464</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/6u8mo3rm' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/6u8mo3rm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:20<00:00,  5.78it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.13it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.09it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 23.02it/s]\n",
      "100%|██████████| 118/118 [00:16<00:00,  7.04it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 22.86it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.89it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.45it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.21it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.48it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.50it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.07it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.23it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.88it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.08it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.94it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.16it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.29it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.43it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.50it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.32it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.26it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.45it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.36it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.22it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.24it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.65it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.82it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.96it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.90it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.73it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.23it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.49it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.55it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.20it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.06it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.38it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.45it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.24it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.52it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.68it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.11it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.96it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.30it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.56it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.26it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.16it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.40it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.25it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.62it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.18it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.23it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.12it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.11it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.39it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.96it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.21it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.60it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.49it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.21it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.35it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.20it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.80it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.15it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.83it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.65it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.04it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.26it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.16it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.23it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.54it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.51it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.42it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.32it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.14it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.82it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.63it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.09it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.97it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.94it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.08it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.14it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.32it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.52it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.64it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 17.75it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.12it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.26it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.16it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.43it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.12it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.32it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.04it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.30it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.58it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.25it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.36it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.25it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.64it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.25it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.51it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.41it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.98it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.18it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.97it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.26it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.70it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.37it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.00it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.90it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.89it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.23it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.50it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.53it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.41it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.83it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.46it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.01it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.46it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.91it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.32it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.89it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.70it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.78it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.03it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.40it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.06it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.46it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.53it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.36it/s]\n",
      "100%|██████████| 150/150 [51:30<00:00, 20.60s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>149</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>loss</td><td>0.58061</td></tr><tr><td>step</td><td>17699</td></tr><tr><td>val_loss</td><td>0.61709</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-glitter-464</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/6u8mo3rm' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/6u8mo3rm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_163614-6u8mo3rm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 200\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 150, \"lr\": 0.0001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacategory = \"D\"\n",
    "\n",
    "datatype = f\"experiment_2S_{datacategory}\"\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = pickle.load(open(f\"{data_path}/train_data_{datacategory}.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{data_path}/valid_data_{datacategory}.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data[\"inputs\"]+valid_data[\"inputs\"])\n",
    "\n",
    "ctx_len = len(train_data[\"inputs\"][0])\n",
    "\n",
    "trainset = LMDataset(train_data[\"inputs\"], labels=train_data[\"inputs\"])\n",
    "valset = LMDataset(valid_data[\"inputs\"], labels=valid_data[\"inputs\"])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240613_172803-maliyys9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/maliyys9' target=\"_blank\">feasible-forest-465</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/maliyys9' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/maliyys9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:17<00:00,  6.97it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 22.10it/s]\n",
      "100%|██████████| 125/125 [00:18<00:00,  6.63it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.85it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.16it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.93it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.88it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.29it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.32it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.82it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.33it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.46it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.31it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.73it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.32it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.89it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.26it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.26it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.35it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.12it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.29it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.52it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.37it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.32it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.56it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.23it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.30it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.91it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.31it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.09it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.09it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.31it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.81it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.32it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.26it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.50it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.20it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.68it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.32it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.06it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.14it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.71it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.73it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.22it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.69it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.32it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.26it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.09it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.96it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.25it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.36it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.31it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.41it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.80it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.00it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.27it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.06it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.30it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.56it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.28it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.35it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.35it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.48it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.25it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.82it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.24it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.59it/s]\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.24it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.04it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.39it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.28it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.47it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.72it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.35it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.43it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.81it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.39it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.41it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.09it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.29it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.59it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.85it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.66it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.36it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.44it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.29it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.10it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.29it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.38it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.94it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.71it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.42it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.53it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.45it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.20it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.04it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.33it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.30it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.93it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.66it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.03it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.29it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.45it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.31it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.29it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.50it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.09it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.46it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.84it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.83it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.35it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.35it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.70it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.91it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.69it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.41it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.45it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.45it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.98it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.10it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.81it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.61it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.07it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.25it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.51it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.46it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.67it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.45it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.99it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.32it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.40it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.31it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 22.01it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.07it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.62it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.28it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.75it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.21it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.59it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.67it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.40it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.84it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.87it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.46it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.69it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.38it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.98it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.66it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.79it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.29it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.75it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.76it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.47it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.77it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.12it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.39it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.49it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.65it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.11it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.55it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.69it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.92it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.78it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.55it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.13it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.06it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.27it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.30it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.36it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.72it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.46it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.05it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.44it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.85it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.38it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.67it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.28it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.43it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.87it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.42it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.65it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.46it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.86it/s]\n",
      "100%|██████████| 125/125 [00:19<00:00,  6.43it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 21.62it/s]\n",
      "100%|██████████| 150/150 [52:59<00:00, 21.20s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>149</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>loss</td><td>0.55696</td></tr><tr><td>step</td><td>18749</td></tr><tr><td>val_loss</td><td>0.57309</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-forest-465</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/maliyys9' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/maliyys9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_172803-maliyys9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 200\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 150, \"lr\": 0.0001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacategory = \"F\"\n",
    "\n",
    "datatype = f\"experiment_2S_{datacategory}\"\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = pickle.load(open(f\"{data_path}/train_data_{datacategory}.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{data_path}/valid_data_{datacategory}.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data[\"inputs\"]+valid_data[\"inputs\"])\n",
    "\n",
    "ctx_len = len(train_data[\"inputs\"][0])\n",
    "\n",
    "trainset = LMDataset(train_data[\"inputs\"], labels=train_data[\"inputs\"])\n",
    "valset = LMDataset(valid_data[\"inputs\"], labels=valid_data[\"inputs\"])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240613_182118-c32uygnr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/c32uygnr' target=\"_blank\">vivid-wind-466</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/c32uygnr' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/c32uygnr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:16<00:00,  6.96it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.64it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.42it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.47it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.30it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.53it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.45it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.82it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.62it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.45it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.69it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.47it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.51it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.65it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.48it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.52it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.26it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.48it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.11it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.06it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.50it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.46it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.86it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.41it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.26it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.84it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.28it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.36it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.61it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.59it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.80it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.47it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.68it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.30it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.46it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.86it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.88it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.78it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.07it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.12it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.94it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.47it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.28it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.80it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.99it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.55it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.96it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.15it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.95it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.52it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.76it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.59it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.08it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.19it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.99it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.28it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.91it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.79it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.59it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.51it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.02it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.70it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.67it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.16it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.28it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.72it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.58it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.14it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.75it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.54it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.81it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.69it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.04it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.26it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.53it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.58it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.01it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.75it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.45it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.36it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.89it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.29it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.45it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.59it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.39it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.76it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.68it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.76it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.81it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.42it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.13it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.34it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.80it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.33it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.16it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.98it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.67it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.52it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.66it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.19it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.69it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.75it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.76it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.14it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.67it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.83it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.89it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.00it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.14it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.64it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.62it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.16it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.88it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.23it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.11it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.71it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.92it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.01it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.35it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.36it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.94it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.68it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.27it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.47it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.73it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.99it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.54it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.68it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.08it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.90it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.50it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.23it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.58it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.53it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.23it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.80it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.18it/s]\n",
      "100%|██████████| 150/150 [49:46<00:00, 19.91s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>149</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>loss</td><td>0.56027</td></tr><tr><td>step</td><td>17699</td></tr><tr><td>val_loss</td><td>0.59591</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-wind-466</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/c32uygnr' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/c32uygnr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_182118-c32uygnr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 200\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 150, \"lr\": 0.0001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacategory = \"G\"\n",
    "\n",
    "datatype = f\"experiment_2S_{datacategory}\"\n",
    "data_path = f\"{path}/experiment_2S\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = pickle.load(open(f\"{data_path}/train_data_{datacategory}.pkl\", 'rb'))\n",
    "valid_data = pickle.load(open(f\"{data_path}/valid_data_{datacategory}.pkl\", 'rb'))\n",
    "\n",
    "vocab_size = get_vocab_size(train_data[\"inputs\"]+valid_data[\"inputs\"])\n",
    "\n",
    "ctx_len = len(train_data[\"inputs\"][0])\n",
    "\n",
    "trainset = LMDataset(train_data[\"inputs\"], labels=train_data[\"inputs\"])\n",
    "valset = LMDataset(valid_data[\"inputs\"], labels=valid_data[\"inputs\"])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_type = \"M1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240613_191122-1o06rca5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/1o06rca5' target=\"_blank\">fragrant-elevator-468</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/1o06rca5' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/1o06rca5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:16<00:00,  7.11it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.58it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.78it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.07it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.70it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.97it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.06it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.90it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.69it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.79it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.15it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.56it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.31it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.04it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.19it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.33it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.09it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.75it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.68it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.63it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.38it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.09it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.18it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.64it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.65it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.19it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.90it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.71it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.98it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.94it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.80it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.69it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.43it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.37it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.52it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.91it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.61it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.46it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.69it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.43it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.67it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.80it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.04it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.83it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.53it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.68it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.23it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.53it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.50it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.68it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.19it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.62it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.92it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.04it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.60it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.59it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.27it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.56it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.62it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.84it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.18it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.77it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.65it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.27it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.12it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.58it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.64it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.66it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.05it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.57it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.31it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.61it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.69it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.20it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.67it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.51it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.60it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.61it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.00it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.64it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.83it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.57it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.58it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.68it/s]\n",
      "100%|██████████| 118/118 [00:17<00:00,  6.59it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.39it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.48it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.64it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.63it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.51it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.49it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.50it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.19it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.78it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.55it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.12it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.82it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.12it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.43it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.87it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.15it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.92it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 16.03it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.12it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.26it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.74it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.59it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.04it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.99it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.29it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.79it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.40it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.17it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.22it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.83it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.06it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.47it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.97it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.88it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.57it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.90it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.02it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.11it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.25it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.89it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.94it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.44it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.74it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.14it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.09it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.43it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.07it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.49it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.20it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.84it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.39it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.02it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.40it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.44it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.29it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.34it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.01it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.45it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.24it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.95it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.52it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.30it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.78it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.49it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.10it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.14it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.16it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.38it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.34it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 21.02it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.27it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.72it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.80it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.92it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.13it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.60it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.40it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.72it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 19.98it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.32it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 20.23it/s]\n",
      "100%|██████████| 150/150 [50:27<00:00, 20.19s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>149</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>loss</td><td>0.6195</td></tr><tr><td>step</td><td>17699</td></tr><tr><td>val_loss</td><td>0.64355</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-elevator-468</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/1o06rca5' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/1o06rca5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_191122-1o06rca5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = vocab_size + 5 #for special tokens\n",
    "num_labels = 200\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "# model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "model = GPT2LMHeadModel._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 150, \"lr\": 0.0001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4vq32h5g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>██▆▆▆▆▅▅▅▄▃▅▃▄▃▂▂▄▃▄▂▃▂▃▃▁▃▁▂▄▂▂▂▂▃▂▂▁▂▃</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▃▃▂▂▂▂▂▁▂▁▁▂▂▁▂▁▁▁▂▂▂▁▂▂▂▁▁▁▃▂▂▃▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>107</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>loss</td><td>0.29507</td></tr><tr><td>step</td><td>17055</td></tr><tr><td>val_loss</td><td>0.53149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-elevator-478</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/4vq32h5g' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/4vq32h5g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240614_142241-4vq32h5g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4vq32h5g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/evlab/jackking/modular_transformers/modular_transformers/train/wandb/run-20240614_144215-jo1mbh5w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/modular_transformers/input%20statistics/runs/jo1mbh5w' target=\"_blank\">amber-puddle-480</a></strong> to <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/modular_transformers/input%20statistics' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/jo1mbh5w' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/jo1mbh5w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:09<00:00, 15.96it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 55.39it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.08it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 55.40it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 55.33it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.05it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 55.22it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.06it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 55.18it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 51.62it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.73it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.76it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.51it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.85it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.89it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.41it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.99it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.35it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.38it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.07it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.51it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.33it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.96it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.52it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.00it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.90it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.26it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.40it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.79it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.88it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.48it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.95it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.16it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.65it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.86it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.00it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.30it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.44it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.75it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.28it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.79it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.83it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.85it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.91it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.86it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.77it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.00it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.66it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.82it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.55it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.40it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.52it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.50it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.32it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.81it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.97it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.20it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.11it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.01it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.38it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.98it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.39it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.32it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.95it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.63it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.19it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.44it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.32it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.87it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.85it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.44it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.78it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.42it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 51.22it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.96it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.74it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.96it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.46it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.76it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.86it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.00it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.89it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.98it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.53it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.75it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.98it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.83it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.79it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.76it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.32it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.63it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.29it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.68it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.29it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.82it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.62it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.76it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.70it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.22it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.82it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.65it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.88it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.68it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.00it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.81it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.88it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.70it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.19it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.50it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.64it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.30it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.42it/s]\n",
      "100%|██████████| 157/157 [00:10<00:00, 15.69it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.36it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.30it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.41it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.79it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.71it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.96it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.84it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.04it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.39it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.57it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.35it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.36it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.06it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.38it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.85it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.88it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.50it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.85it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.63it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.56it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.35it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.75it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.63it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.06it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.68it/s]\n",
      "100%|██████████| 157/157 [00:10<00:00, 15.69it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.97it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.91it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.16it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.56it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.89it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.58it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.78it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.92it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.40it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.76it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.78it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.33it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.30it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.27it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.25it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 51.18it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.38it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.24it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.75it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.28it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.20it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.24it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.36it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.33it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.15it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.05it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.76it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.27it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.48it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.29it/s]\n",
      "100%|██████████| 157/157 [00:10<00:00, 15.70it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.32it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.05it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.88it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.75it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.05it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.53it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.79it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.82it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.35it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.43it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.03it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.33it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.79it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.06it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.93it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.82it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.35it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.83it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 52.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.20it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.36it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.73it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.32it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.85it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.38it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.77it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.96it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.52it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.29it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.08it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.41it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.33it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.90it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.21it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.34it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.78it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.81it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.94it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.44it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.73it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.12it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.58it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.88it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.56it/s]\n",
      "100%|██████████| 157/157 [00:10<00:00, 15.68it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.47it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.81it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.80it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 53.60it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.87it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 51.24it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 15.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s]\n",
      "100%|██████████| 200/200 [35:35<00:00, 10.68s/it]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▇███▇▇█▇▆▇▆▆▄▆▅▅▅▆▄▆▄▅▃▂▄▃▄▄▂▂▃▂▄▃▅▃▄▄▄▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>███▇▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>loss</td><td>0.59589</td></tr><tr><td>step</td><td>31399</td></tr><tr><td>val_loss</td><td>0.53425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-puddle-480</strong> at: <a href='https://wandb.ai/modular_transformers/input%20statistics/runs/jo1mbh5w' target=\"_blank\">https://wandb.ai/modular_transformers/input%20statistics/runs/jo1mbh5w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240614_144215-jo1mbh5w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "datatype = \"experiment_1\"\n",
    "datacategory = \"EF\"\n",
    "\n",
    "train_data_1 = pickle.load(open(f\"{path}/{datatype}/train_data_{datacategory}1.pkl\", \"rb\"))\n",
    "val_data_1 = pickle.load(open(f\"{path}/{datatype}/valid_data_{datacategory}1.pkl\", \"rb\"))\n",
    "train_data_2 = pickle.load(open(f\"{path}/{datatype}/train_data_{datacategory}2.pkl\", \"rb\"))\n",
    "val_data_2 = pickle.load(open(f\"{path}/{datatype}/valid_data_{datacategory}2.pkl\", \"rb\"))\n",
    "\n",
    "train_data = train_data_1 + train_data_2\n",
    "val_data = val_data_1 + val_data_2\n",
    "\n",
    "len1 = len(train_data_1)\n",
    "len2 = len(train_data_2)\n",
    "train_labels = [0]*len1 + [1]*len2\n",
    "len1 = len(val_data_1)\n",
    "len2 = len(val_data_2)\n",
    "val_labels = [0]*len1 + [1]*len2\n",
    "\n",
    "ctx_len = len(train_data[0])\n",
    "\n",
    "vocab_size = get_vocab_size(train_data+val_data)\n",
    "\n",
    "trainset = LMDataset(train_data, labels=torch.tensor(train_labels))\n",
    "valset = LMDataset(val_data, labels=torch.tensor(val_labels))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "model_type = f\"M3_{datacategory}\"\n",
    "\n",
    "embedding_dim = 128\n",
    "n_layer = 12\n",
    "n_head = 4\n",
    "resid_pdrop = 0.1\n",
    "embd_pdrop = 0.2\n",
    "attn_pdrop = 0.2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "num_labels = 2\n",
    "\n",
    "model_config = GPT2Config(n_layer = n_layer, n_head = n_head, n_embd = embedding_dim, n_positions = ctx_len, #vocab_size = vocab_size,\n",
    "                          resid_pdrop=resid_pdrop, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, num_labels=num_labels\n",
    "                          )\n",
    "model = GPT2ForSequenceClassification._from_config(model_config)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "model_name = f\"{datatype}/{model_type}_{embedding_dim}_{n_layer}\"\n",
    "train_config = {\"num_epochs\": 200, \"lr\": 0.0000001, \"lr_scheduler\": lr_scheduler, \"batch_size\": batch_size, \"resid_pdrop\": resid_pdrop, \"embd_pdrop\": embd_pdrop, \"n_head\": n_head,\n",
    "                \"attn_pdrop\": attn_pdrop, \"model_name\": model_name, \"model_type\": model_type, \"embedding_dim\": embedding_dim, \"n_layer\": n_layer, \"ctx_len\": ctx_len, \"datatype\": datatype}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config[\"lr\"])\n",
    "if train_config[\"lr_scheduler\"] is not None:\n",
    "    if train_config[\"lr_scheduler\"] == \"cosine_annealing\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"]*30)\n",
    "    elif train_config[\"lr_scheduler\"] == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_config[\"num_epochs\"] * len(trainloader))\n",
    "    elif train_config[\"lr_scheduler\"] == \"step\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.98)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, train_config, model_name, trainloader, valloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
