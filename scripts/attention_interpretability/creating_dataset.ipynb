{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x154fd8161d60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "from modular_transformers.straightening.straightening_utils import compute_model_activations, compute_model_curvature\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "path = \"/om2/user/jackking/modular_transformers/scripts/attention_interpretability\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split By Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = \"ud\"\n",
    "sentence_length = 10\n",
    "datatype = f\"{datasource}/{sentence_length}_word\"\n",
    "model_name = \"gpt2-xl\"\n",
    "\n",
    "data_dir = \"/rdma/vast-rdma/vast/evlab/ehoseini/MyData/sent_sampling/analysis/straightening/generation/sentences_ud_sentencez_token_filter_v3_textNoPeriod_cntx_3_cont_7.pkl\"\n",
    "with open(data_dir, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "sentences = []\n",
    "for i, raw_sentence in enumerate(data):\n",
    "    sentence = tokenizer.encode(raw_sentence)\n",
    "    if len(sentence) < sentence_length:\n",
    "        pass\n",
    "    sentence = sentence[:sentence_length]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "activations = compute_model_activations(model, sentences, device)\n",
    "curvatures = compute_model_curvature(activations)\n",
    "\n",
    "full_path = f\"{path}/data/{datatype}\"\n",
    "\n",
    "if not os.path.exists(full_path):\n",
    "    os.makedirs(full_path)\n",
    "\n",
    "with open(f\"{full_path}/{model_name}_activations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(activations, f)\n",
    "\n",
    "with open(f\"{full_path}/{model_name}_curvatures.pkl\", \"wb\") as f:\n",
    "    pickle.dump(curvatures, f)\n",
    "\n",
    "with open(f\"{full_path}/sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split By Sentence Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect All Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
    "fw_text = fw.select_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "two_word_sets = []\n",
    "for i, sample in enumerate(fw_text):\n",
    "    sample = tokenizer.encode(sample[\"text\"])\n",
    "    #randomly extract 10 pairs of words\n",
    "    for j in range(10):\n",
    "        start = np.random.randint(0, len(sample)-2)\n",
    "        two_word_sets.append(sample[start:start+2])\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if len(two_word_sets) >= 100000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 10\n",
    "datasource = \"fineweb\"\n",
    "datatype = f\"{datasource}/{sentence_length}_word\"\n",
    "num_sentences = 10000\n",
    "\n",
    "sentences = []\n",
    "for i, sample in enumerate(fw_text):\n",
    "    #pull out any sentences are of length 10\n",
    "    sample = sent_tokenize(sample[\"text\"])\n",
    "    sents = [sentence for sentence in sample if len(tokenizer.encode(sentence)) == sentence_length]\n",
    "    if sents:\n",
    "        sentences.extend(sents)\n",
    "    if len(sentences) > num_sentences:\n",
    "        break\n",
    "\n",
    "full_path = f\"{path}/data/{datatype}\"\n",
    "\n",
    "if not os.path.exists(full_path):\n",
    "    os.makedirs(full_path)\n",
    "\n",
    "with open(f\"{full_path}/all_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate full context and two token suprisal for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 10\n",
    "datasource = \"fineweb\"\n",
    "datatype = f\"{datasource}/{sentence_length}_word\"\n",
    "full_path = f\"{path}/data/{datatype}\"\n",
    "\n",
    "with open(f\"{full_path}/all_sentences.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.72it/s]\n",
      "100%|██████████| 157/157 [05:07<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.inputs[idx]\n",
    "        return inputs\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", max_length=sentence_length, truncation=True)[\"input_ids\"]\n",
    "dataset = SentenceDataset(inputs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# finding the suprisal at each token given the full previous context\n",
    "\n",
    "def get_whole_context_surprisals(model, dataloader, device):\n",
    "    model.eval()\n",
    "    surprisals = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        inputs = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        log_probs = -F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        for i in range(inputs.shape[0]):\n",
    "            #find the surprisal of each place in the context predicting the next token\n",
    "            token_indices = inputs[i, 1:].cpu().numpy()\n",
    "            token_log_probs = log_probs[i, torch.arange(len(token_indices)), token_indices]\n",
    "            surprisals.append(token_log_probs.cpu().numpy())\n",
    "\n",
    "    return surprisals\n",
    "\n",
    "whole_context_surprisals = get_whole_context_surprisals(model, dataloader, device)\n",
    "\n",
    "with open(f\"{full_path}/whole_context_surprisals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(whole_context_surprisals, f)\n",
    "\n",
    "# finding the surprisal at each token given only the previous two tokens\n",
    "\n",
    "def get_two_token_context_surprisals(model, dataloader, device):\n",
    "    model.eval()\n",
    "    surprisals = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        #get every three token pair\n",
    "        # shape: (batch_size, max_length-2, 3)\n",
    "        batch = np.stack([batch[:, i:i+3] for i in range(sentence_length - 2)], axis=1)\n",
    "        for sample in batch:\n",
    "            inputs = torch.tensor(sample).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "            log_probs = -F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            one_sentence = []\n",
    "\n",
    "            for i in range(inputs.shape[0]):\n",
    "                #for each three token string\n",
    "                token_index = inputs[i, -1].cpu().numpy() #get the last token\n",
    "                token_log_prob = log_probs[i, -2, token_index] #prob of the second to last token predicting the last token\n",
    "                one_sentence.append(token_log_prob.item())\n",
    "\n",
    "            surprisals.append(one_sentence)\n",
    "\n",
    "    return surprisals\n",
    "\n",
    "two_token_context_surprisals = get_two_token_context_surprisals(model, dataloader, device)\n",
    "\n",
    "with open(f\"{full_path}/two_token_context_surprisals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(two_token_context_surprisals, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sentences with correct statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 10\n",
    "datasource = \"fineweb\"\n",
    "datatype = f\"{datasource}/{sentence_length}_word\"\n",
    "full_path = f\"{path}/data/{datatype}\"\n",
    "\n",
    "whole_context_surprisals = pickle.load(open(f\"{full_path}/whole_context_surprisals.pkl\", \"rb\"))\n",
    "two_token_surprisals = pickle.load(open(f\"{full_path}/two_token_context_surprisals.pkl\", \"rb\"))\n",
    "sentences = pickle.load(open(f\"{full_path}/all_sentences.pkl\", \"rb\"))\n",
    "\n",
    "whole_context_surprisals = np.array(whole_context_surprisals)\n",
    "two_token_surprisals = np.array(two_token_surprisals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_sentences = []\n",
    "whole_slow_surprisals = []\n",
    "two_token_slow_surprisals = []\n",
    "fast_sentences = []\n",
    "whole_fast_surprisals = []\n",
    "two_token_fast_surprisals = []\n",
    "\n",
    "all_indivd_suprisals = []\n",
    "for surprisal in whole_context_surprisals:\n",
    "    all_indivd_suprisals.extend(surprisal)\n",
    "all_indivd_suprisals = np.array(all_indivd_suprisals)\n",
    "whole_suprisal_mean = np.mean(all_indivd_suprisals)\n",
    "whole_surprisal_std = np.std(all_indivd_suprisals)\n",
    "\n",
    "all_indivd_suprisals = []\n",
    "for surprisal in two_token_surprisals:\n",
    "    all_indivd_suprisals.extend(surprisal)\n",
    "all_indivd_suprisals = np.array(all_indivd_suprisals)\n",
    "two_token_suprisal_mean = np.mean(all_indivd_suprisals)\n",
    "two_token_surprisal_std = np.std(all_indivd_suprisals)\n",
    "\n",
    "\n",
    "def check_for_fast(whole_context_surprisal, two_token_surprisal):\n",
    "    #whole context surprisal is high and limited (two token) context is low\n",
    "    clause1 = np.all(whole_context_surprisal > whole_suprisal_mean - whole_surprisal_std*4/5) \n",
    "    clause2 = np.all(two_token_surprisal < (two_token_suprisal_mean + two_token_surprisal_std / 2))\n",
    "\n",
    "    return clause1 and clause2\n",
    "                                                                \n",
    "def check_for_slow(whole_context_surprisal, two_token_surprisal):\n",
    "    # the last two tokens are low surprisal with full context\n",
    "    clause1 = np.all(whole_context_surprisal[-2:] < whole_suprisal_mean - whole_surprisal_std / 3)\n",
    "\n",
    "    # the third to last token is also low surprisal with full context\n",
    "    clause3 = np.all(whole_context_surprisal[-4:-2] < whole_suprisal_mean)\n",
    "\n",
    "    #in general the model does not have fast statistics\n",
    "    clause2 = np.all(two_token_surprisal > two_token_suprisal_mean - two_token_surprisal_std*5/8) \n",
    "\n",
    "    #this is especially true for the last few tokens, because their prediction needs to come from slow context\n",
    "    clause4 = np.all(two_token_surprisal[-3:] > two_token_suprisal_mean - two_token_surprisal_std*2/3)\n",
    "\n",
    "    return clause1 and clause2 and clause3 and clause4\n",
    "\n",
    "for sentence, whole_context_surprisal, two_token_surprisal in zip(sentences, whole_context_surprisals, two_token_surprisals):\n",
    "\n",
    "    if check_for_slow(whole_context_surprisal, two_token_surprisal):\n",
    "        slow_sentences.append(sentence)\n",
    "        whole_slow_surprisals.append(whole_context_surprisal)\n",
    "        two_token_slow_surprisals.append(two_token_surprisal)\n",
    "\n",
    "    elif check_for_fast(whole_context_surprisal, two_token_surprisal):\n",
    "        fast_sentences.append(sentence)\n",
    "        whole_fast_surprisals.append(whole_context_surprisal)\n",
    "        two_token_fast_surprisals.append(two_token_surprisal)\n",
    "\n",
    "whole_fast_surprisals = np.array(whole_fast_surprisals)\n",
    "two_token_fast_surprisals = np.array(two_token_fast_surprisals)\n",
    "whole_slow_surprisals = np.array(whole_slow_surprisals)\n",
    "two_token_slow_surprisals = np.array(two_token_slow_surprisals)\n",
    "\n",
    "with open(f\"{full_path}/slow_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(slow_sentences, f)\n",
    "\n",
    "with open(f\"{full_path}/whole_slow_surprisals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(whole_slow_surprisals, f)\n",
    "\n",
    "with open(f\"{full_path}/two_token_slow_surprisals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(two_token_slow_surprisals, f)\n",
    "\n",
    "with open(f\"{full_path}/fast_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fast_sentences, f)\n",
    "\n",
    "with open(f\"{full_path}/whole_fast_surprisals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(whole_fast_surprisals, f)\n",
    "\n",
    "with open(f\"{full_path}/two_token_fast_surprisals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(two_token_fast_surprisals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(whole_fast_surprisals), np.mean(two_token_fast_surprisals), np.mean(whole_slow_surprisals), np.mean(two_token_slow_surprisals)\n",
    "np.mean(whole_fast_surprisals, axis=0), np.mean(whole_slow_surprisals, axis=0)\n",
    "np.mean(two_token_fast_surprisals, axis=0), np.mean(two_token_slow_surprisals, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 10\n",
    "datasource = \"fineweb\"\n",
    "datatype = f\"{datasource}/{sentence_length}_word\"\n",
    "full_path = f\"{path}/data/{datatype}\"\n",
    "\n",
    "slow_sentences = pickle.load(open(f\"{full_path}/slow_sentences.pkl\", \"rb\"))\n",
    "fast_sentences = pickle.load(open(f\"{full_path}/fast_sentences.pkl\", \"rb\"))\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "model.to(device)\n",
    "\n",
    "fast_sentences = tokenizer(fast_sentences, return_tensors=\"pt\", max_length=sentence_length, truncation=True)[\"input_ids\"]\n",
    "activations = compute_model_activations(model, fast_sentences, device)\n",
    "fast_curvature = compute_model_curvature(activations)\n",
    "\n",
    "slow_sentences = tokenizer(slow_sentences, return_tensors=\"pt\", max_length=sentence_length, truncation=True)[\"input_ids\"]\n",
    "activations = compute_model_activations(model, slow_sentences, device)\n",
    "slow_curvature = compute_model_curvature(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_curve = np.mean(fast_curvature[\"curve\"], axis=1) / np.pi * 180\n",
    "slow_curve = np.mean(slow_curvature[\"curve\"], axis=1) / np.pi * 180\n",
    "fast_curve_std = np.std(fast_curvature[\"curve\"], axis=1) / np.pi * 180\n",
    "slow_curve_std = np.std(slow_curvature[\"curve\"], axis=1) / np.pi * 180\n",
    "plt.plot(fast_curve, label=\"fast\")\n",
    "plt.plot(slow_curve, label=\"slow\")\n",
    "plt.fill_between(np.arange(len(fast_curve)), fast_curve - fast_curve_std, fast_curve + fast_curve_std, alpha=0.5)\n",
    "plt.fill_between(np.arange(len(slow_curve)), slow_curve - slow_curve_std, slow_curve + slow_curve_std, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature over last four tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_curve = np.mean(np.mean(fast_curvature[\"all_layer_curve_all\"], axis=0)[:, -4:], axis = -1) / np.pi * 180\n",
    "slow_curve = np.mean(np.mean(slow_curvature[\"all_layer_curve_all\"], axis=0)[:, -4:], axis = -1) / np.pi * 180\n",
    "fast_curve_std = np.std(fast_curvature[\"curve\"], axis=1) / np.pi * 180\n",
    "slow_curve_std = np.std(slow_curvature[\"curve\"], axis=1) / np.pi * 180\n",
    "plt.plot(fast_curve, label=\"fast\")\n",
    "plt.plot(slow_curve, label=\"slow\")\n",
    "plt.fill_between(np.arange(len(fast_curve)), fast_curve - fast_curve_std, fast_curve + fast_curve_std, alpha=0.5)\n",
    "plt.fill_between(np.arange(len(slow_curve)), slow_curve - slow_curve_std, slow_curve + slow_curve_std, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature at Each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(7, 1, figsize=(8, 20))\n",
    "for i in range(1, 8):\n",
    "    curve = np.mean(np.mean(slow_curvature[\"all_layer_curve_all\"], axis=0)[:, i-1:i], axis = -1) / np.pi * 180\n",
    "    axs[i-1].plot(curve, label=\"slow\")\n",
    "    curve = np.mean(np.mean(fast_curvature[\"all_layer_curve_all\"], axis=0)[:, i-1:i], axis = -1) / np.pi * 180\n",
    "    axs[i-1].plot(curve, label=\"fast\")\n",
    "    axs[i-1].set_title(f\"Curve {i}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
