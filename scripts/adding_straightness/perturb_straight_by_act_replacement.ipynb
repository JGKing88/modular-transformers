{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 158.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 32.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal analyzed\n",
      "0.1\n",
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "sentences generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 155.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 32.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "straighter analyzed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 160.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curved analyzed\n",
      "['and we don\\'t want to be the ones to tell you that you\\'re wrong.\\n\\nThe problem is that the \"', 'Tolstoi, who was born in Russia, was a member of the Russian Academy of Sciences and a member of the', \"Being well-informed about the issues is a good start. But it's not enough. We need to be able to communicate\", \"Of course, you can't just go out and buy a new car. You have to get a loan. And that's\", 'The clans didn\\'t know what to do. They were all in shock.\\n\\n\"I\\'m sorry, but I can', 'I am half way through the book and I am loving it. I am also a huge fan of the series and I am', \"She still has some of the same problems she had before, but she's not as bad as she was before. She's\", 'Neither me nor my wife have ever been to the hospital. I have never been to the hospital. I have never been to', \"But the words did not come from the mouth of a man who was in the least degree of the Lord's favor.\\n\", 'Unfortunately, at its core, the problem is that the government is not doing enough to help people who are struggling to make ends', 'This section pertains to the use of the term \"citizen\" in the context of the Immigration and Refugee Protection Act.', 'Until Europe wakes up to the fact that the US is not the only superpower in the world, the US will be forced to', 'He was incredibly learned and had a great sense of humor. He was a great guy.\"\\n\\nThe family is asking for', \"They require a lot of work, but they're worth it.\\n\\nThe first step is to get a good idea of\", 'She stole the information from the database and used it to create a fake ID.\\n\\n\"She was able to get a', 'They helped about three or four people get out of the building,\" said one witness.\\n\\nThe fire was reported at about', \"Clean store, friendly staff, and a great selection of beer.\\n\\nI've been coming here for years and I've\", 'If you have children, you may want to consider a home-based daycare.\\n\\nIf you have a disability,', \"You also don't want to be too aggressive with your attacks, as you'll be taking damage from the enemy.\\n\\n\", 'That was evident in the first half of the season, when the team was a mess. The team was in a state of', 'You can also remove the \"S\" from the end of the name.\\n\\nFor example, if you want to remove', 'When she scraped the last of the snow off her face, she saw the man\\'s face.\\n\\n\"I\\'m', \"Just remember to make sure you have a good balance of carbs and protein.\\n\\nIf you're not sure what to eat\", 'They wanted no excuse to leave the country. They wanted to be able to go back to their families and live normal lives.\"', 'Mr Weasley was slumped against the wall, his eyes closed, his face buried in his hands.\\n\\n\"I\\'m sorry', 'I believe I undertook a very important task in the last few days. I have been able to see the world through the eyes', 'It is in many ways a very simple game. You have to move your mouse around the screen to move the ball. The', 'Dallas Criminal Attorney Peter J. Kline, who has represented the family of the victim, said the family is \"devast', 'He later gave in to the pressure and said he would not be able to attend the event.\\n\\n\"I\\'m not', \"Just let me know if you have any questions.\\n\\nThanks,\\n\\n-Mike\\n\\nPS: I'm not\", 'Walt Whitman, \"Song of Myself,\" 1892\\n\\n\"The Song of Myself\" is a poem by', 'You were just in the right place at the right time. You were just in the right place at the right time.\\n', \"King snakes sometimes stop eating for a few days, but they usually resume eating within a few days.\\n\\nThe snake's\", 'This costs absolutely nothing.\\n\\nThe only thing you need to do is to download the app and install it on your phone', 'Have a good trip!\\n\\n-Travis<|endoftext|>The first time I saw the word \"cisgender\" was in', 'The First World War was a war of the nations, not of the peoples. It was a war of the nations, not', \"It was born to be a great game, and it's still a great game. It's just that it's not as\", 'Fort Lee is bordered by the New Jersey Turnpike and the New York State Thruway.\\n\\nThe Port', 'Pour the soy sauce over the chicken and toss to coat.\\n\\nPlace the chicken on a plate and pour the sauce', 'Somewhere in the middle of the night, a man in a black suit and a white shirt walked into the room.', 'The bungling of the investigation into the death of a young woman in a car crash in the early hours of the morning has', 'Even that locution is not enough to satisfy the court. The court says that the \"proper\" way to describe the', 'And sampling the local brews is a great way to get a taste of the local culture.\\n\\n\"I think it', 'Parking spaces are limited, so please plan accordingly.\\n\\nThe event is free and open to the public.\\n\\n', 'Birds arrived sooner than expected, and the first birds were seen in the early morning. The first birds were seen in the', \"I am bringing two of my favorite things together: the world of the comic book and the world of the movie. I'm\", 'And it has, in fact, been a long time since the last time the U.S. government has been so openly', 'Any US attack on Syria would be a \"game changer\" in the region, he said.\\n\\n\"If we', 'The Arabs have shown themselves to be the most intelligent and the most capable of all the peoples of the world. They have shown', 'There has been much speculation about the future of the company, which has been in the news for the past few months.\\n']\n",
      "15 post done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed, AutoModel, AutoConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from minicons import scorer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import json\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "from modular_transformers.straightening.straightening_utils import compute_model_activations, compute_model_curvature\n",
    "from modular_transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "\n",
    "from modular_transformers.models import components\n",
    "from transformer_xray.perturb_utils import register_pertubation_hooks\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "max_len = 25\n",
    "layer_num = 48\n",
    "embedding_size = 1600\n",
    "first_sequence_len = 4\n",
    "\n",
    "principal_dimensions_for_curved = 10\n",
    "principal_dimensions_for_straight = 1600\n",
    "\n",
    "#set seed\n",
    "set_seed(42)\n",
    "\n",
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "def perturb_input(input, hook, perturbations, principal_dimensions):\n",
    "    perturb_idx = input.shape[1] - 1\n",
    "\n",
    "    input[:, perturb_idx, :] = input[:, perturb_idx, :] + perturbations.to(device)\n",
    "\n",
    "    return input\n",
    "\n",
    "def get_curvature(P1, P2, P3):\n",
    "    v1 = P2 - P1\n",
    "    v2 = P3 - P2\n",
    "    v1 = v1 / v1.norm(dim=-1, keepdim=True)\n",
    "    v2 = v2 / v2.norm(dim=-1, keepdim=True)\n",
    "    curvature = torch.acos(torch.sum(v1 * v2, dim=-1))\n",
    "    return curvature\n",
    "\n",
    "curve_difs = {0.1: None, 0.25: None, 0.5: None, 0.75: None, 1: None}\n",
    "\n",
    "def get_perturbations(model, data, perturb_location, size):\n",
    "    model.reset_hooks()\n",
    "\n",
    "    num_samples = 10\n",
    "\n",
    "    straightest_perturbations = torch.zeros(len(data), embedding_size)\n",
    "    curviest_perturbations = torch.zeros(len(data), embedding_size)\n",
    "        \n",
    "    def record_perturbation_directions(input, hook):\n",
    "        perturb_idx = input.shape[1] - 1\n",
    "\n",
    "        random_directions = torch.randn(num_samples, input.shape[0], input.shape[2]).to(device)\n",
    "        cloned_input = input.clone()\n",
    "        norm = (cloned_input[:, perturb_idx, :] - cloned_input[:, perturb_idx-1, :]).norm(dim=-1, keepdim=True)\n",
    "        random_perturbations = (random_directions / random_directions.norm(dim=-1, keepdim=True)) * norm.view(1, -1, 1) * size * 0.4\n",
    "         \n",
    "        norm = cloned_input[:, perturb_idx, :].norm(dim=-1, keepdim=True)\n",
    "        new_points = cloned_input[:, perturb_idx, :].clone().unsqueeze(0) + random_perturbations\n",
    "        new_points = new_points / new_points.norm(dim=-1, keepdim=True) * norm.view(1, -1, 1)\n",
    "\n",
    "        perturbations_curves = torch.zeros(num_samples, input.shape[0]).to(device)\n",
    "        for i, new_point in enumerate(new_points):\n",
    "            curvature = get_curvature(cloned_input[:, perturb_idx-2, :], cloned_input[:, perturb_idx-1, :], new_point)\n",
    "            perturbations_curves[i] = curvature\n",
    "                \n",
    "        min_indices = torch.argmin(perturbations_curves, dim=0)\n",
    "        for i in range(input.shape[0]):\n",
    "            straightest_perturbations[i, :] = random_perturbations[min_indices[i], i, :]\n",
    "\n",
    "        max_indices = torch.argmax(perturbations_curves, dim=0)\n",
    "        for i in range(input.shape[0]):\n",
    "            curviest_perturbations[i, :] = random_perturbations[max_indices[i], i, :]\n",
    "        \n",
    "    fwd_hooks = [\n",
    "        (perturb_location, record_perturbation_directions)\n",
    "    ]\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        data,\n",
    "        return_type=None,\n",
    "        fwd_hooks=fwd_hooks,\n",
    "    )\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return straightest_perturbations, curviest_perturbations\n",
    "\n",
    "def generate_perturbed_token(model, data, perturb_function):\n",
    "\n",
    "    sequence_len = data.shape[1]\n",
    "\n",
    "    post_activations = torch.zeros((len(data), layer_num, sequence_len, embedding_size))\n",
    "    def record_post_activations(input, hook, layer):\n",
    "        post_activations[:, layer, :, :] = input\n",
    "\n",
    "    fwd_hooks = []\n",
    "    \n",
    "    fwd_hooks.append((\n",
    "            perturb_location,\n",
    "            perturb_function\n",
    "        ))\n",
    "    \n",
    "    for layer in range(layer_num):\n",
    "        fwd_hooks.append((utils.get_act_name(\"resid_post\", layer), partial(record_post_activations, layer=layer)))\n",
    "\n",
    "    logits = model.run_with_hooks(\n",
    "        data,\n",
    "        return_type=\"logits\",\n",
    "        fwd_hooks=fwd_hooks\n",
    "    )\n",
    "\n",
    "    new_token = logits.argmax(dim=-1)[:, -1]\n",
    "\n",
    "    return new_token, post_activations\n",
    "\n",
    "def continued_gen_normal(model, data, length):\n",
    "    #generate new sentences by adding new token to the end of the sentence\n",
    "    final_data = torch.zeros((len(data), max_len), dtype=torch.int64).to(device)\n",
    "\n",
    "    batch_size = 500\n",
    "    batch_indxs = torch.arange(0, len(data), batch_size)\n",
    "    for i in range(len(batch_indxs) - 1):\n",
    "        batch = data[batch_indxs[i]:batch_indxs[i+1]]\n",
    "\n",
    "        for _ in range(length):\n",
    "            logits = model(batch).logits\n",
    "            new_token = logits.argmax(dim=-1)[:, -1]\n",
    "            batch = torch.cat([batch, new_token.unsqueeze(1)], dim=1)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        final_data[batch_indxs[i]:batch_indxs[i+1], :] = batch.type(torch.int64)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def generate_normal_sentences(data):\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2-xl\", device=device)\n",
    "    \n",
    "    normal_data = data.clone()\n",
    "\n",
    "    new_token, activations = generate_perturbed_token(model, normal_data, perturb_function = lambda input, hook: None)\n",
    "    normal_data = torch.cat([normal_data, new_token.unsqueeze(1)], dim=1)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    del model\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\").to(device)\n",
    "    model.output_logits = True\n",
    "\n",
    "    length = max_len - first_sequence_len - num_perturbations\n",
    "    normal_data = continued_gen_normal(model, normal_data, length)\n",
    "        \n",
    "    return normal_data, activations\n",
    "\n",
    "def continued_gen_perturbed(model, data, activations, perturb_location, length):\n",
    "    #generate new sentences by adding new token to the end of the sentence\n",
    "    final_data = torch.zeros((len(data), max_len), dtype=torch.int64).to(device)\n",
    "\n",
    "    if perturb_location == \"blocks.15.hook_resid_post\":\n",
    "        layer = 15 + 1\n",
    "    elif perturb_location == \"blocks.5.hook_resid_post\":\n",
    "        layer = 5 + 1\n",
    "    elif perturb_location == \"blocks.30.hook_resid_post\":\n",
    "        layer = 30 + 1\n",
    "\n",
    "    def replace_activations(input, hook):\n",
    "        input[:, 0:first_sequence_len, :] = activations[:, layer, :, :]\n",
    "        return input\n",
    "\n",
    "    for _ in range(length):\n",
    "        logits = model.run_with_hooks(\n",
    "            data,\n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=[(perturb_location, replace_activations)]\n",
    "        )\n",
    "        new_token = logits.argmax(dim=-1)[:, -1]\n",
    "        data = torch.cat([data, new_token.unsqueeze(1)], dim=1)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_sentences(data, perturb_location, size):\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2-xl\", device=device)\n",
    "    \n",
    "    straighter_data = data.clone()\n",
    "    curved_data = data.clone()\n",
    "\n",
    "    straightest_perturbations, curviest_perturbations = get_perturbations(model, straighter_data, perturb_location, size)\n",
    "    perturb_function = partial(perturb_input, perturbations = straightest_perturbations, principal_dimensions=principal_dimensions_for_straight)\n",
    "    new_token, straighter_activations = generate_perturbed_token(model, straighter_data, perturb_function)\n",
    "\n",
    "    straightest_perturbations, curviest_perturbations = get_perturbations(model, curved_data, perturb_location, size)\n",
    "    perturb_function = partial(perturb_input, perturbations = curviest_perturbations, principal_dimensions=principal_dimensions_for_curved)\n",
    "    new_token, curved_activations = generate_perturbed_token(model, curved_data, perturb_function)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    length = max_len - first_sequence_len\n",
    "    straighter_data = continued_gen_perturbed(model, straighter_data, straighter_activations, perturb_location, length)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    curved_data = continued_gen_perturbed(model, curved_data, curved_activations, perturb_location, length)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return straighter_data, curved_data, straighter_activations, curved_activations\n",
    "\n",
    "\n",
    "def record_activations(model, data):\n",
    "    post_activations = torch.zeros((len(data), layer_num, max_len, embedding_size))\n",
    "    def record_post_activations(input, hook, layer):\n",
    "        post_activations[:, layer, :, :] = input\n",
    "\n",
    "    fwd_hooks = []\n",
    "    for layer in range(layer_num):\n",
    "        fwd_hooks.append((utils.get_act_name(\"resid_post\", layer), partial(record_post_activations, layer=layer)))\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        data, \n",
    "        return_type=None, \n",
    "        fwd_hooks=fwd_hooks,\n",
    "    )\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return post_activations\n",
    "\n",
    "\n",
    "def run_perturbed(gen_data, gen_activations):\n",
    "    #gen activations shape: (num_sentences, num_layers, num_tokens, hidden_size)\n",
    "    gen_curvatures = [{}] * num_perturbations\n",
    "\n",
    "    gen_curvatures[0] = compute_model_curvature(gen_activations)\n",
    "        \n",
    "    #get curvature with sentences\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2-xl\", device=device)\n",
    "    data_activations = record_activations(model,gen_data)\n",
    "    data_curvature = {}\n",
    "    data_curvature[\"post\"] = compute_model_curvature(data_activations)\n",
    "\n",
    "    #get surprisal with sentences\n",
    "    data_decoded = [tokenizer.decode(sentence) for sentence in gen_data]\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\").to(device)\n",
    "    model = scorer.IncrementalLMScorer(model, tokenizer=tokenizer, device=device)\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(data_decoded), batch_size):\n",
    "        data_decoded_batch = data_decoded[i:i+batch_size]\n",
    "        surprisals = torch.tensor(model.sequence_score(data_decoded_batch, reduction = lambda x: -x.sum(0)))\n",
    "        if i == 0:\n",
    "            surprisals_all = surprisals\n",
    "        else:\n",
    "            surprisals_all = torch.cat([surprisals_all, surprisals], dim=0)\n",
    "\n",
    "    return_dict = {\n",
    "        \"surprisals\": surprisals_all,\n",
    "        \"sentences\": data_decoded,\n",
    "        \"curvatures\": data_curvature,\n",
    "        \"gen_curvatures\": gen_curvatures\n",
    "    }\n",
    "    return return_dict\n",
    "\n",
    "def launch(data, perturb_location):\n",
    "    \n",
    "    path_to_dict = f\"/om2/user/jackking/modular_transformers/scripts/adding_straightness/perturb_straight_byact_results_{perturb_location}.pkl\"\n",
    "    # if os.path.exists(path_to_dict):\n",
    "    #     new_surprisals = pickle.load(open(path_to_dict, \"rb\"))\n",
    "    # else:\n",
    "    new_surprisals = {}\n",
    "        \n",
    "    data = data[:50]\n",
    "    cut_data = data[:, :first_sequence_len].to(device)\n",
    "\n",
    "    normal_data, normal_activations = generate_normal_sentences(cut_data)\n",
    "    normal_results = run_perturbed(normal_data, normal_activations)\n",
    "    print(\"normal analyzed\")\n",
    "\n",
    "    new_surprisals[\"normal\"] = normal_results\n",
    "\n",
    "    for size in [0.1]:\n",
    "        if size in new_surprisals:\n",
    "            continue\n",
    "        print(size)        \n",
    "        straighter_data, curved_data, straighter_activations, curved_activations = generate_sentences(cut_data, perturb_location, size)\n",
    "        print(\"sentences generated\")\n",
    "        \n",
    "        straighter_results = run_perturbed(straighter_data, straighter_activations)\n",
    "        print(\"straighter analyzed\")\n",
    "        curved_results = run_perturbed(curved_data, curved_activations)\n",
    "        print(\"curved analyzed\")\n",
    "\n",
    "        new_surprisals[size] = {\n",
    "            \"straighter\": straighter_results,\n",
    "            \"curved\": curved_results\n",
    "        }\n",
    "\n",
    "        print(new_surprisals[0.1][\"straighter\"][\"sentences\"])\n",
    "\n",
    "        with open(path_to_dict, 'wb') as f:\n",
    "            pickle.dump(new_surprisals, f)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"/rdma/vast-rdma/vast/evlab/ehoseini/MyData/sent_sampling/analysis/straightening/generation/sentences_ud_sentencez_token_filter_v3_textNoPeriod_cntx_3_cont_7.pkl\"\n",
    "    with open(data_dir, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    data = tokenizer.batch_encode_plus(data, add_special_tokens=True, padding='longest', return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    num_perturbations = 1\n",
    "\n",
    "    perturb_location = \"blocks.15.hook_resid_post\"\n",
    "    launch(data, perturb_location)\n",
    "    print(\"15 post done\")\n",
    "\n",
    "    # perturb_location = \"blocks.5.hook_resid_post\"\n",
    "    # launch(data, perturb_location)\n",
    "    # print(\"5 post done\")\n",
    "\n",
    "    # perturb_location = \"blocks.30.hook_resid_post\"\n",
    "    # launch(data, perturb_location)\n",
    "    # print(\"30 post done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
