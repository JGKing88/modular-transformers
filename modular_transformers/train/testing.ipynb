{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import accelerate\n",
    "from accelerate import (Accelerator,\n",
    "                        DeepSpeedPlugin)\n",
    "from accelerate.utils import (LoggerType, DummyOptim, DummyScheduler)\n",
    "from transformers import (AdamW,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                         get_cosine_schedule_with_warmup,\n",
    "                          set_seed,\n",
    "                          AutoConfig,\n",
    "                          GPT2LMHeadModel)\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from modular_transformers.train.utils import Group_Texts\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from modular_transformers.models.gpt2.utils import initialize_gpt2_weights\n",
    "import pickle\n",
    "\n",
    "from modular_transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from modular_transformers.models import components\n",
    "from modular_transformers.models.loss_utils import l2_reg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "MAX_GPU_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "CONTEXT_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate(model, eval_dataloader, accelerator):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            batch = torch.stack(batch['input_ids']).transpose(1, 0)\n",
    "            outputs = model(batch, labels=batch)\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    accelerator.print(f\"validation loss: {loss.item()}, validation perplexity {perplexity.item()}\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_dim = 128\n",
    "normalize_loss = False\n",
    "logit_multiplier = 1\n",
    "loss_hooks = {1: \"l2_curvature\"}\n",
    "n_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Get the state dictionary\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Save the state dictionary\n",
    "torch.save(state_dict, 'gpt2_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = {'regsize': 768, 'vocab_size': len(tokenizer), 'n_ctx': CONTEXT_LENGTH, 'bos_token_id': tokenizer.bos_token_id,\n",
    "                    'eos_token_id': tokenizer.eos_token_id, \"bottleneck\": 768, \"n_layer\": 12, \"loss_hooks\": None, \"normalize_loss\": normalize_loss,\n",
    "                    \"logit_multiplier\": logit_multiplier, \"inter_multiplier\": 1, \"n_heads\": 12}\n",
    "                    \n",
    "config = GPT2Config(config)\n",
    "model = components.LM(config)\n",
    "model_state_dict = model.state_dict()\n",
    "state_dict = torch.load('gpt2_weights.pt')\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n",
      "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(state_dict.keys())\n",
    "print(model_state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set checkpoint if needed ------------------------------------------------\n",
    "chkpoint = None\n",
    "wandb_id = \"amvcstc4\"\n",
    "epoch_buffer = 12\n",
    "\n",
    "#Set training config --------------------------------------------\n",
    "\n",
    "data='10M'\n",
    "batch_size = 64\n",
    "\n",
    "train_config = {\"lr\": 0.0006, \"num_epochs\": 20, \"correct_bias\": True, \"seed\": 42, \"batch_size\": batch_size}\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "path = '/om/weka/evlab/ehoseini/MyData/miniBERTa_v2/'\n",
    "grouped_pad_train = load_from_disk(\n",
    "    os.path.join(path, f'miniBERTa-{data}-crunched',\n",
    "                    f'train_context_len_{CONTEXT_LENGTH}'))\n",
    "grouped_pad_valid = load_from_disk(\n",
    "    os.path.join(path, f'miniBERTa-{data}-crunched',\n",
    "                    f'valid_context_len_{CONTEXT_LENGTH}'))\n",
    "\n",
    "# If the batch size is too big we use gradient accumulation\n",
    "gradient_accumulation_steps = 1\n",
    "if train_config['batch_size'] > MAX_GPU_BATCH_SIZE:\n",
    "    gradient_accumulation_steps = train_config['batch_size'] // MAX_GPU_BATCH_SIZE\n",
    "    batch_size = MAX_GPU_BATCH_SIZE\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "else:\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "eval_dataloader = DataLoader(grouped_pad_valid, shuffle=False, batch_size=EVAL_BATCH_SIZE)\n",
    "train_dataloader = DataLoader(grouped_pad_train, shuffle=True, batch_size=batch_size)\n",
    "del grouped_pad_train, grouped_pad_valid\n",
    "\n",
    "#set model config ---------------------------------------\n",
    "\n",
    "config = {'regsize': 128, 'vocab_size': len(tokenizer), 'n_ctx': CONTEXT_LENGTH, 'bos_token_id': tokenizer.bos_token_id,\n",
    "                    'eos_token_id': tokenizer.eos_token_id, \"bottleneck\": bottleneck_dim, \"n_layer\": n_layer, \"loss_hooks\": loss_hooks, \"normalize_loss\": normalize_loss,\n",
    "                    \"logit_multiplier\": logit_multiplier, \"inter_multiplier\": 1, \"n_heads\": 4}\n",
    "                    \n",
    "config = GPT2Config(config)\n",
    "model = components.LM(config)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = model.to(accelerator.device)\n",
    "\n",
    "# Define optimizer\n",
    "# Creates Dummy Optimizer if `optimizer` was specified in the config file else creates AdamW Optimizer\n",
    "optimizer_cls = (torch.optim.AdamW\n",
    "    if accelerator.state.deepspeed_plugin is None\n",
    "        or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
    "    else DummyOptim\n",
    ")\n",
    "optimizer = optimizer_cls(params=model.parameters(), lr=train_config['lr'])\n",
    "if (\n",
    "        accelerator.state.deepspeed_plugin is None\n",
    "        or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
    "):\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=(len(train_dataloader) * train_config['num_epochs']),\n",
    "    )\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "# Pass everything to accelerator\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n",
    "\n",
    "data_count = 0\n",
    "absolute_step = 0\n",
    "\n",
    "real_epochs = 7\n",
    "\n",
    "for epoch in tqdm(range(real_epochs)):\n",
    "\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        batch = [torch.stack(batch[x]).transpose(1, 0) for x in ['input_ids', 'attention_mask']]\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(batch[0], labels=batch[0], attention_mask=batch[1])\n",
    "            logit_loss = outputs.loss\n",
    "\n",
    "            if loss_hooks is not None:\n",
    "                extra_losses = model.output_extra_losses()\n",
    "                extra_loss = 0\n",
    "                for loss in extra_losses.values():\n",
    "                    if loss is not None:\n",
    "                        extra_loss += loss\n",
    "                                    \n",
    "                logit_loss = logit_loss * logit_multiplier\n",
    "                extra_loss = extra_loss\n",
    "                loss = logit_loss + extra_loss\n",
    "                logit_loss = logit_loss.item()\n",
    "                extra_loss = extra_loss.item()\n",
    "            else:\n",
    "                loss = logit_loss\n",
    "                            \n",
    "            accelerator.backward(loss)\n",
    "            lr_scheduler.step()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        data_count += batch[0].shape[0]\n",
    "\n",
    "        absolute_step += 1\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "accelerator.end_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hooks = {4: \"l0_curvature\"}\n",
    "config = {'regsize': 768, 'vocab_size': len(tokenizer), 'n_ctx': 1024, 'bos_token_id': tokenizer.bos_token_id,\n",
    "                    'eos_token_id': tokenizer.eos_token_id, \"bottleneck\": 768, \"n_layer\": 12, \"loss_hooks\": loss_hooks, \"normalize_loss\": False,\n",
    "                    \"logit_multiplier\": 1, \"inter_multiplier\": 1, \"n_heads\": 12, \"pretrained\": False, \"warmup\": False}\n",
    "                    \n",
    "config = GPT2Config(config)\n",
    "model = components.LM(config)\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "path = '/om/weka/evlab/ehoseini/MyData/miniBERTa_v2/'\n",
    "data_size = \"10M\"\n",
    "data = load_from_disk(\n",
    "    os.path.join(path, f'miniBERTa-{data_size}-crunched',\n",
    "                    f'train_context_len_{1024}'))\n",
    "\n",
    "dataloader = DataLoader(data, shuffle=True, batch_size=2)\n",
    "batch = next(iter(dataloader))\n",
    "batch = [torch.stack(batch[x]).transpose(1, 0) for x in ['input_ids', 'attention_mask']]\n",
    "outputs = model(batch[0], labels=batch[0], attention_mask=batch[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
